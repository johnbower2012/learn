{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Here I've coded an artificial neural network with an arbitrary number of layers. The ANN is encoded in a class, and is fed a dictionary of your choices for activation functions and loss functions, as well as a list of the neurons in each layer, for example a two layer network with 100 and 50, respectively, is fed as [100,50].\n",
    "\n",
    "Then, the easiest way to search for effective parameters sets is to use RunOne or RunTwo. RunOne will train a one layer NN across the list of options you feed it, then test its accuracy. For example, [10,20,30] will train with 10 neurons, 20 neurons, 30 neurons, and spit out the accuracy for each. Similarly, RunTwo trains in a list of two lists, the first for the number of neurons in the first layer and the second for the second layer. It will train across all combinations and sit out a matrix of accuracies.\n",
    "\n",
    "Then, once you've identified a good parameter set, use the solitary Train function grouped with RunOne and RunTwo in order to train one a specific parameter set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "def read_dataset(feature_file, label_file):\n",
    "    ''' Read data set in *.csv to data frame in Pandas'''\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values # convert values in dataframe to numpy array (features)\n",
    "    y = df_y.values # convert values in dataframe to numpy array (label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def normalize_features(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler #import libaray\n",
    "    scaler = StandardScaler() # call an object function\n",
    "    scaler.fit(X_train) # calculate mean, std in X_train\n",
    "    X_train_norm = scaler.transform(X_train) # apply normalization on X_train\n",
    "    X_test_norm = scaler.transform(X_test) # we use the same normalization on X_test\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "\n",
    "def one_hot_encoder(y_train, y_test):\n",
    "    ''' convert label to a vector under one-hot-code fashion '''\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_train)\n",
    "    y_train_ohe = lb.transform(y_train)\n",
    "    y_test_ohe = lb.transform(y_test)\n",
    "    return y_train_ohe, y_test_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, X, y, DACT, DLOSS, filt=[(8,1,3,3)], stride=[1], hidden_layer=[100], lr=0.01):\n",
    "        self.X = X # features\n",
    "        self.y = y # labels (targets) in one-hot-encoder\n",
    "        self.imag_dim = np.sqrt(X.shape[1])\n",
    "        \n",
    "        self.activation = DACT['F']\n",
    "        self.derivative = DACT['D']\n",
    "        \n",
    "        self.lossfunction = DLOSS['F']\n",
    "        self.lossderivative = DLOSS['D']\n",
    "        \n",
    "        self.filter = filt\n",
    "        self.regularization_parameter = DLOSS['RP']\n",
    "        self.hidden_layer = hidden_layer # number of neuron in the hidden layer\n",
    "        \n",
    "        # In this example, we only consider 1 hidden layer\n",
    "        self.lr = lr # learning rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.in_nn = X.shape[1] # number of neurons in the inpute layer              \n",
    "        self.out_nn = y.shape[1]\n",
    "        \n",
    "        self.f[]\n",
    "        for i in range(0,len(self.filter):\n",
    "            self.f.append(InitializeFilter(self.filter[i]))\n",
    "                    \n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.W.append(np.random.randn(self.in_nn,self.hidden_layer[0])/np.sqrt(self.in_nn))\n",
    "        self.b.append(np.zeros((1,self.hidden_layer[0])))\n",
    "        for i in range(0,len(self.hidden_layer)-1):\n",
    "            self.W.append(np.random.randn(self.hidden_layer[i],self.hidden_layer[i+1])/np.sqrt(self.hidden_layer[i]))\n",
    "            self.b.append(np.zeros((1,self.hidden_layer[i+1])))\n",
    "\n",
    "        self.W.append(np.random.randn(self.hidden_layer[-1],self.out_nn)/np.sqrt(self.hidden_layer[-1]))\n",
    "        self.b.append(np.zeros((1,self.out_nn)))\n",
    "        self.C = [self.W,self.b]\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        # hidden layer\n",
    "        self.z = []\n",
    "        self.f = []\n",
    "        ## z_1 = xW_1 + b_1\n",
    "        self.\n",
    "        self.z.append(np.dot(self.X, self.W[0]) + self.b[0])\n",
    "        ## activation function :  f_1 = \\tanh(z_1)\n",
    "        self.f.append(self.activation(self.z[0]))\n",
    "        for i in range(0,len(self.hidden_layer)-1):\n",
    "            self.z.append(np.dot(self.f[i], self.W[i+1]) + self.b[i+1])\n",
    "            self.f.append(self.activation(self.z[-1]))\n",
    "        # output layer\n",
    "        ## z_2 = f_1W_2 + b_2\n",
    "        self.z.append(np.dot(self.f[-1], self.W[-1]) + self.b[-1])\n",
    "        #\\hat{y} = softmax}(z_2)$\n",
    "        self.y_hat = softmax(self.z[-1])\n",
    "        \n",
    "        \n",
    "    def back_propagation(self):\n",
    "        d = []\n",
    "        d.insert(0,self.lossderivative(self.y,self.y_hat,self.regularization_parameter,self.C))\n",
    "        dW = []\n",
    "        db = []\n",
    "        for i in range(len(self.hidden_layer)):\n",
    "            dW.insert(0,self.f[-1-i].T.dot(d[0]))\n",
    "            db.insert(0,np.sum(d[0],axis=0,keepdims=True))\n",
    "            d.insert(0,self.derivative(self.f[-1-i])*(d[0].dot(self.W[-1-i].T)))\n",
    "        dW.insert(0,self.X.T.dot(d[0]))\n",
    "        db.insert(0,np.sum(d[0],axis=0,keepdims=True))\n",
    "        \n",
    "        \n",
    "        # Update the gradident descent\n",
    "        if(self.regularization_parameter==None):\n",
    "            for i in range(len(self.W)):            \n",
    "                self.W[i] = self.W[i] - self.lr * dW[i]\n",
    "                self.b[i] = self.b[i] - self.lr * db[i]\n",
    "        else:\n",
    "            for i in range(len(self.W)):\n",
    "                self.W[i] = self.W[i] - self.lr * dW[i]\n",
    "                if(self.W[i].all()!=0):\n",
    "                    self.W[i] = self.W[i] - self.lr * self.W[i]/np.sqrt(np.sum(self.W[i]*self.W[i]))\n",
    "                self.b[i] = self.b[i] - self.lr * db[i]\n",
    "                if(self.b[i].all()!=0):\n",
    "                    self.b[i] = self.b[i] - self.lr * self.b[i]/np.sqrt(np.sum(self.b[i]*self.b[i]))\n",
    "\n",
    "    def callloss(self):\n",
    "        #  $L = -\\sum_n\\sum_{i\\in C} y_{n, i}\\log(\\hat{y}_{n, i})$\n",
    "        # calculate y_hat\n",
    "        self.feed_forward()\n",
    "        self.loss = self.lossfunction(self.y,self.y_hat,self.regularization_parameter,self.C)\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Use feed forward to calculat y_hat_test\n",
    "        # hidden layer\n",
    "        ## z_1 = xW_1 + b_1\n",
    "        z = np.dot(X_test, self.W[0]) + self.b[0]\n",
    "        for i in range(1,len(self.hidden_layer)+1):\n",
    "            f = self.activation(z)\n",
    "            z = np.dot(f,self.W[i]) + self.b[i]\n",
    "        y_hat_test = softmax(z)\n",
    "\n",
    "        # the rest is similar to the logistic regression\n",
    "        labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        num_test_samples = X_test.shape[0]\n",
    "        # find which index gives us the highest probability\n",
    "        ypred = np.zeros(num_test_samples, dtype=int)\n",
    "        for i in range(num_test_samples):\n",
    "            ypred[i] = labels[np.argmax(y_hat_test[i,:])]\n",
    "        return ypred\n",
    "    \n",
    "    def train(self,epochs):\n",
    "        for i in range(epochs):\n",
    "            self.feed_forward()\n",
    "            self.back_propagation()\n",
    "            self.callloss()\n",
    "        \n",
    "def softmax(z):\n",
    "    exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True)) # for stablility\n",
    "    # keepdims = True means that the output's dimension is the same as of z\n",
    "    softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "    return softmax_scores\n",
    "\n",
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(1347, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "X_train_digits, y_train_digits = read_dataset('Digits_X_train.csv', 'Digits_y_train.csv')\n",
    "X_test_digits, y_test_digits = read_dataset('Digits_X_test.csv', 'Digits_y_test.csv')\n",
    "X_train_norm_digits, X_test_norm_digits = normalize_features(X_train_digits, X_test_digits)\n",
    "y_train_ohe_digits, y_test_ohe_digits = one_hot_encoder(y_train_digits, y_test_digits)\n",
    "\n",
    "print(X_train_digits.shape)\n",
    "X_train_digits = np.reshape(X_train_digits,(-1,8,8))\n",
    "print(X_train_digits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def InitializeFilter(dimensions):\n",
    "    return np.random.normal(loc=0,scale=1.0/(np.sqrt(np.prod(dimensions))),size=dimensions)\n",
    "\n",
    "\n",
    "def Convolution(images, filt, bias, activation, stride_i=1,stride_j=1):\n",
    "    (num_filt, num_ch_f, f_i, f_j) = filt.shape\n",
    "    (num_imag, num_ch, imag_i, imag_j) = images.shape\n",
    "    \n",
    "    if(num_ch_f != num_ch):\n",
    "        print(\"Big mistake! The number of channels in the filter does not match the number of channels in the images!\")\n",
    "\n",
    "    out_i = int(np.ceil((imag_i - f_i)/stride_i) + 1)\n",
    "    out_j = int(np.ceil((imag_j - f_j)/stride_j) + 1)\n",
    "    z = np.zeros((num_imag, num_filt, out_i, out_j))\n",
    "    i=0\n",
    "    for image in images:\n",
    "        for curr_f in range(num_filt):\n",
    "            curr_y = curr_y_end = out_y = 0\n",
    "            f_y_end = f_j + 1\n",
    "            while(out_y < out_j):\n",
    "                curr_x = curr_x_end = out_x = 0\n",
    "                curr_y_end = curr_y + f_j\n",
    "                if(curr_y_end > imag_j):\n",
    "                    curr_y_end = imag_j\n",
    "                    f_y_end = curr_y_end - curr_y\n",
    "                f_x_end = f_i + 1\n",
    "                while(out_x < out_i):\n",
    "                    curr_x_end = curr_x + f_i\n",
    "                    if(curr_x_end > imag_i):\n",
    "                        curr_x_end = imag_i\n",
    "                        f_x_end = curr_x_end - curr_x\n",
    "                    z[i,curr_f,out_x,out_y] = np.sum(image[:,curr_x:curr_x_end,curr_y:curr_y_end]*filt[curr_f,:,0:f_x_end,0:f_y_end]) + bias[curr_f]\n",
    "                    curr_x += stride_i\n",
    "                    out_x += 1\n",
    "                curr_y += stride_j\n",
    "                out_y += 1\n",
    "        i+=1\n",
    "    return z,activation(z)\n",
    "\n",
    "def MaxPool(images,pool_i=2,pool_j=2,stride_i=2,stride_j=2):\n",
    "    (num_imag, num_ch, imag_i, imag_j) = images.shape\n",
    "\n",
    "    out_i = int(np.ceil((imag_i - pool_i)/stride_i)) + 1\n",
    "    out_j = int(np.ceil((imag_j - pool_j)/stride_j)) + 1\n",
    "    z = np.zeros((num_imag, num_ch, out_i, out_j))\n",
    "    index = np.zeros((num_imag, num_ch, out_i, out_j))\n",
    "    \n",
    "    i=0\n",
    "    for image in images:\n",
    "        for curr_chan in range(num_ch):\n",
    "            curr_y = out_y = 0\n",
    "            while(out_y < out_j):\n",
    "                curr_x = out_x = 0\n",
    "                curr_y_end = curr_y + pool_j\n",
    "                if(curr_y_end > imag_j):\n",
    "                    current_y_end = imag_j\n",
    "                while(out_x < out_i):\n",
    "                    curr_x_end = curr_x + pool_i\n",
    "                    if(curr_x_end > imag_i):\n",
    "                        curr_x_end = imag_i\n",
    "                    z[i,curr_chan,out_x,out_y] = np.max(image[curr_chan,curr_x:curr_x_end,curr_y:curr_y_end])\n",
    "                    index[i,curr_chan,out_x,out_y] = np.argmax(image[curr_chan,curr_x:curr_x_end,curr_y:curr_y_end])\n",
    "                    curr_x += stride_i\n",
    "                    out_x +=1\n",
    "                curr_y += stride_j\n",
    "                out_y += 1\n",
    "        i+=1\n",
    "    return z,index\n",
    "\n",
    "def Connection(images,weights,bias,activation):\n",
    "    z = images.dot(weights) + bias\n",
    "    return z, activation(z)\n",
    "\n",
    "def backConvolution(dprev,conv_in,filt,derivative,stride_i=1,stride_j=1):\n",
    "    (num_filt, num_ch_f, f_i, f_j) = filt.shape\n",
    "    (num_imag, num_ch, imag_i, imag_j) = conv_in.shape\n",
    "       \n",
    "    dout = np.zeros((num_ch,imag_i,imag_j))\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dbias = np.zeros((num_filt,1))\n",
    "    \n",
    "    for conv in conv_in:\n",
    "        dout_conv = np.zeros((num_ch,imag_i,imag_j))\n",
    "        for curr_f in range(num_filt):\n",
    "            curr_y = out_y = 0\n",
    "            f_y_end = f_i\n",
    "            while(curr_y + f_j < imag_j + 1):\n",
    "                curr_x = out_x = 0\n",
    "                curr_y_end = curr_y + f_j\n",
    "                if(curr_y_end > imag_j):\n",
    "                    curr_y_end = imag_j\n",
    "                    f_y_end = curr_y_end - curr_y\n",
    "                f_x_end = f_j\n",
    "                while(curr_x + f_i < imag_i + 1): \n",
    "                    curr_x_end = curr_x + f_i\n",
    "                    if(curr_x_end > imag_i):\n",
    "                        curr_x_end = imag_i\n",
    "                        f_x_end = curr_x_end - curr_i\n",
    "                    dout_conv[:,curr_x:curr_x_end,curr_y:curr_y_end] += dprev[curr_f, out_x, out_y] * filt[curr_f,:,:f_x_end,:f_y_end]\n",
    "                    dfilt[curr_f,:,:f_x_end,:f_y_end] += dprev[curr_f,out_x,out_y] * conv[:,curr_x:curr_x_end,curr_y:curr_y_end]\n",
    "\n",
    "                    curr_x += stride_i\n",
    "                    out_x +=1\n",
    "                curr_y += stride_j\n",
    "                out_y += 1\n",
    "            dbias[curr_f] = np.sum(dprev[curr_f])\n",
    "        dout += dout_conv*derivative(conv)\n",
    "\n",
    "    return dout,dfilt,dbias\n",
    "\n",
    "\n",
    "\n",
    "#def Convolution(images, filt, bias, activation, stride_i=1,stride_j=1):\n",
    "\n",
    "\n",
    "def linear(X):\n",
    "    return X\n",
    "def RELU(X):\n",
    "    a = np.copy(X)\n",
    "    a[X<0]=0\n",
    "    return a\n",
    "def RELU_dx(X):\n",
    "    dx = np.zeros(X.shape)\n",
    "    dx[X>0] = 1\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprev:\n",
      " [[[1. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[2. 0. 0. 2.]\n",
      "  [2. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [2. 0. 0. 0.]]]\n",
      "conv:\n",
      " [[[[1.  0.  0.  0.  1.3]\n",
      "   [1.2 0.  0.  0.  0. ]\n",
      "   [0.  0.  0.  0.  0. ]\n",
      "   [0.  0.  0.  0.  0. ]\n",
      "   [1.7 0.  0.  0.  0. ]]\n",
      "\n",
      "  [[1.  0.  0.  0.  1. ]\n",
      "   [1.  0.  0.  0.  0. ]\n",
      "   [0.  0.  0.  0.  0. ]\n",
      "   [0.  0.  0.  0.  0. ]\n",
      "   [1.  0.  0.  0.  0. ]]]]\n",
      "f:\n",
      " [[[[1.  1. ]\n",
      "   [1.  1. ]]\n",
      "\n",
      "  [[1.  0. ]\n",
      "   [0.  1. ]]]\n",
      "\n",
      "\n",
      " [[[0.7 0.5]\n",
      "   [0.3 0.6]]\n",
      "\n",
      "  [[0.  1. ]\n",
      "   [1.  0. ]]]]\n",
      "d:\n",
      " [[[2.4 0.  0.  0.  2. ]\n",
      "  [4.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0. ]\n",
      "  [1.6 0.  0.  0.  0. ]]\n",
      "\n",
      " [[1.  0.  0.  0.  2. ]\n",
      "  [3.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0. ]\n",
      "  [2.  0.  0.  0.  0. ]]]\n",
      "df:\n",
      " [[[[2.2 1.3]\n",
      "   [2.9 0. ]]\n",
      "\n",
      "  [[2.  1. ]\n",
      "   [2.  0. ]]]\n",
      "\n",
      "\n",
      " [[[4.4 2.6]\n",
      "   [5.8 0. ]]\n",
      "\n",
      "  [[4.  2. ]\n",
      "   [4.  0. ]]]]\n",
      "db:\n",
      " [[4.]\n",
      " [8.]]\n"
     ]
    }
   ],
   "source": [
    "dprev = np.zeros((2,4,4))\n",
    "dprev[0,0,0] = 1\n",
    "dprev[0,1,0] = 1\n",
    "dprev[0,0,3] = 1\n",
    "dprev[0,3,0] = 1\n",
    "dprev[1,0,0] = 2\n",
    "dprev[1,1,0] = 2\n",
    "dprev[1,0,3] = 2\n",
    "dprev[1,3,0] = 2\n",
    "conv_in = np.zeros((1,2,5,5))\n",
    "conv_in[0,0,0,0] = 1\n",
    "conv_in[0,0,1,0] = 1.2\n",
    "conv_in[0,0,0,4] = 1.3\n",
    "conv_in[0,0,4,0] = 1.7\n",
    "conv_in[0,1,0,0] = 1\n",
    "conv_in[0,1,1,0] = 1\n",
    "conv_in[0,1,0,4] = 1\n",
    "conv_in[0,1,4,0] = 1\n",
    "filt = np.zeros((2,2,2,2))\n",
    "filt[0,0,0,0] = 1\n",
    "filt[0,0,1,1] = 1\n",
    "filt[0,0,0,1] = 1\n",
    "filt[0,0,1,0] = 1\n",
    "filt[1,0,0,0] = 0.7\n",
    "filt[1,0,1,1] = 0.6\n",
    "filt[1,0,0,1] = 0.5\n",
    "filt[1,0,1,0] = 0.3\n",
    "filt[0,1,0,0] = 1\n",
    "filt[0,1,1,1] = 1\n",
    "filt[1,1,0,1] = 1\n",
    "filt[1,1,1,0] = 1\n",
    "dout, dfilt, dbias = backConvolution(dprev,conv_in,filt,RELU_dx,stride_i=1,stride_j=1)\n",
    "print(\"dprev:\\n\",dprev)\n",
    "print(\"conv:\\n\",conv_in)\n",
    "print(\"f:\\n\",filt)\n",
    "print(\"d:\\n\",dout)\n",
    "print(\"df:\\n\",dfilt)\n",
    "print(\"db:\\n\",dbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filt:\n",
      " [[[[1. 0.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [0. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 0.]\n",
      "   [1. 0.]]]]\n",
      "bias:\n",
      " [1.3 1.1]\n",
      "image:\n",
      " [[[[ 0. -1. -2. -3. -4.  0.]\n",
      "   [ 1.  0. -1. -2. -3.  0.]\n",
      "   [ 2.  1.  0. -1. -2.  0.]\n",
      "   [ 3.  2.  1.  0. -1.  0.]\n",
      "   [ 4.  3.  2.  1.  0.  0.]]\n",
      "\n",
      "  [[ 1.  0. -1. -2. -3.  0.]\n",
      "   [ 2.  1.  0. -1. -2.  0.]\n",
      "   [ 3.  2.  1.  0. -1.  0.]\n",
      "   [ 4.  3.  2.  1.  0.  0.]\n",
      "   [ 5.  4.  3.  2.  1.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 1.  0. -1. -2. -3.  0.]\n",
      "   [ 2.  1.  0. -1. -2.  0.]\n",
      "   [ 3.  2.  1.  0. -1.  0.]\n",
      "   [ 4.  3.  2.  1.  0.  0.]\n",
      "   [ 5.  4.  3.  2.  1.  0.]]\n",
      "\n",
      "  [[ 2.  1.  0. -1. -2.  0.]\n",
      "   [ 3.  2.  1.  0. -1.  0.]\n",
      "   [ 4.  3.  2.  1.  0.  0.]\n",
      "   [ 5.  4.  3.  2.  1.  0.]\n",
      "   [ 6.  5.  4.  3.  2.  0.]]]]\n",
      "zc:\n",
      " [[[[ 3.3 -4.7 -5.7]\n",
      "   [11.3  3.3 -1.7]\n",
      "   [10.3  6.3  2.3]]\n",
      "\n",
      "  [[ 4.1  0.1 -3.9]\n",
      "   [ 8.1  4.1  0.1]\n",
      "   [ 1.1  1.1  1.1]]]\n",
      "\n",
      "\n",
      " [[[ 7.3 -0.7 -3.7]\n",
      "   [15.3  7.3  0.3]\n",
      "   [12.3  8.3  4.3]]\n",
      "\n",
      "  [[ 6.1  2.1 -1.9]\n",
      "   [10.1  6.1  2.1]\n",
      "   [ 1.1  1.1  1.1]]]]\n",
      "conv:\n",
      " [[[[ 3.3  0.   0. ]\n",
      "   [11.3  3.3  0. ]\n",
      "   [10.3  6.3  2.3]]\n",
      "\n",
      "  [[ 4.1  0.1  0. ]\n",
      "   [ 8.1  4.1  0.1]\n",
      "   [ 1.1  1.1  1.1]]]\n",
      "\n",
      "\n",
      " [[[ 7.3  0.   0. ]\n",
      "   [15.3  7.3  0.3]\n",
      "   [12.3  8.3  4.3]]\n",
      "\n",
      "  [[ 6.1  2.1  0. ]\n",
      "   [10.1  6.1  2.1]\n",
      "   [ 1.1  1.1  1.1]]]]\n",
      "pool:\n",
      " [[[[11.3  0. ]\n",
      "   [10.3  2.3]]\n",
      "\n",
      "  [[ 8.1  0.1]\n",
      "   [ 1.1  1.1]]]\n",
      "\n",
      "\n",
      " [[[15.3  0.3]\n",
      "   [12.3  4.3]]\n",
      "\n",
      "  [[10.1  2.1]\n",
      "   [ 1.1  1.1]]]]\n",
      "pool_index:\n",
      " [[[[2. 0.]\n",
      "   [0. 0.]]\n",
      "\n",
      "  [[2. 1.]\n",
      "   [0. 0.]]]\n",
      "\n",
      "\n",
      " [[[2. 1.]\n",
      "   [0. 0.]]\n",
      "\n",
      "  [[2. 1.]\n",
      "   [0. 0.]]]]\n",
      "flat:\n",
      " [[11.3  0.  10.3  2.3  8.1  0.1  1.1  1.1]\n",
      " [15.3  0.3 12.3  4.3 10.1  2.1  1.1  1.1]]\n"
     ]
    }
   ],
   "source": [
    "filt = InitializeFilter((2,2,2,2))\n",
    "filt[0,0,0,0] = 1\n",
    "filt[0,0,0,1] = 0\n",
    "filt[0,0,1,0] = 0\n",
    "filt[0,0,1,1] = 1\n",
    "filt[1,0,0,0] = 0\n",
    "filt[1,0,0,1] = 0\n",
    "filt[1,0,1,0] = 1\n",
    "filt[1,0,1,1] = 0\n",
    "filt[0,1,0,0] = 1\n",
    "filt[0,1,0,1] = 0\n",
    "filt[0,1,1,0] = 0\n",
    "filt[0,1,1,1] = 1\n",
    "filt[1,1,0,0] = 0\n",
    "filt[1,1,0,1] = 0\n",
    "filt[1,1,1,0] = 1\n",
    "filt[1,1,1,1] = 0\n",
    "print(\"filt:\\n\",filt)\n",
    "image = np.zeros((2,2,5,6))\n",
    "bias = np.zeros(2)\n",
    "bias[0] = 1.3\n",
    "bias[1] = 1.1\n",
    "print(\"bias:\\n\",bias)\n",
    "for m in range(2):\n",
    "    for k in range(2):\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                image[m,k,i,j] = k+i - j+m\n",
    "print(\"image:\\n\",image)\n",
    "zc,conv = Convolution(image,filt,bias,RELU,stride_i=2,stride_j=2)\n",
    "print(\"zc:\\n\",zc)\n",
    "print(\"conv:\\n\",conv)\n",
    "pool,index = MaxPool(conv)\n",
    "print(\"pool:\\n\",pool)\n",
    "print(\"pool_index:\\n\",index)\n",
    "print(\"flat:\\n\",pool.reshape(pool.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
