{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Here I've coded an artificial neural network with an arbitrary number of layers. The ANN is encoded in a class, and is fed a dictionary of your choices for activation functions and loss functions, as well as a list of the neurons in each layer, for example a two layer network with 100 and 50, respectively, is fed as [100,50].\n",
    "\n",
    "Then, the easiest way to search for effective parameters sets is to use RunOne or RunTwo. RunOne will train a one layer NN across the list of options you feed it, then test its accuracy. For example, [10,20,30] will train with 10 neurons, 20 neurons, 30 neurons, and spit out the accuracy for each. Similarly, RunTwo trains in a list of two lists, the first for the number of neurons in the first layer and the second for the second layer. It will train across all combinations and sit out a matrix of accuracies.\n",
    "\n",
    "Then, once you've identified a good parameter set, use the solitary Train function grouped with RunOne and RunTwo in order to train one a specific parameter set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "def read_dataset(feature_file, label_file):\n",
    "    ''' Read data set in *.csv to data frame in Pandas'''\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values # convert values in dataframe to numpy array (features)\n",
    "    y = df_y.values # convert values in dataframe to numpy array (label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def normalize_features(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler #import libaray\n",
    "    scaler = StandardScaler() # call an object function\n",
    "    scaler.fit(X_train) # calculate mean, std in X_train\n",
    "    X_train_norm = scaler.transform(X_train) # apply normalization on X_train\n",
    "    X_test_norm = scaler.transform(X_test) # we use the same normalization on X_test\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "\n",
    "def one_hot_encoder(y_train, y_test):\n",
    "    ''' convert label to a vector under one-hot-code fashion '''\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_train)\n",
    "    y_train_ohe = lb.transform(y_train)\n",
    "    y_test_ohe = lb.transform(y_test)\n",
    "    return y_train_ohe, y_test_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "X_train_digits, y_train_digits = read_dataset('Digits_X_train.csv', 'Digits_y_train.csv')\n",
    "X_test_digits, y_test_digits = read_dataset('Digits_X_test.csv', 'Digits_y_test.csv')\n",
    "X_train_norm_digits, X_test_norm_digits = normalize_features(X_train_digits, X_test_digits)\n",
    "y_train_ohe_digits, y_test_ohe_digits = one_hot_encoder(y_train_digits, y_test_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVERYTHING BELOW IS TESTING! DIVE IN; THE WATER'S FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFilter:\n",
    "    def __init__(self,info):\n",
    "        self.tag = \"convolution\"\n",
    "        self.dimensions = info.dimensions\n",
    "        self.num_filt = info.dimensions[0]\n",
    "        self.num_ch = info.dimensions[1]\n",
    "        self.f_i = info.dimensions[2]\n",
    "        self.f_j = info.dimensions[3]\n",
    "        self.weight = self.FInitializeFilter()\n",
    "        self.bias = self.FInitializeBias()\n",
    "        self.stride_i = info.stride_i\n",
    "        self.stride_j = info.stride_j\n",
    "        self.activation = info.activation        \n",
    "    def FInitializeFilter(self):\n",
    "        return np.random.normal(loc=0,scale=1.0/(np.sqrt(np.prod(self.dimensions))),size=self.dimensions)\n",
    "    def FInitializeBias(self):\n",
    "        return np.random.normal(loc=0,scale=1.0/(np.sqrt(np.prod(self.num_filt))),size=(1,self.num_filt))\n",
    "    \n",
    "    def FFeedForward(self,images):\n",
    "        (num_imag, num_ch, imag_i, imag_j) = images.shape\n",
    "\n",
    "        if(self.num_ch != num_ch):\n",
    "            print(\"Big mistake! The number of channels in the filter does not match the number of channels in the images!\")\n",
    "\n",
    "        out_i = int(np.ceil((imag_i - self.f_i)/self.stride_i) + 1)\n",
    "        out_j = int(np.ceil((imag_j - self.f_j)/self.stride_j) + 1)\n",
    "        z = np.zeros((num_imag, self.num_filt, out_i, out_j))\n",
    "        i=0\n",
    "        for image in images:\n",
    "            for curr_f in range(self.num_filt):\n",
    "                curr_y = curr_y_end = out_y = 0\n",
    "                f_y_end = self.f_j\n",
    "                while(out_y < out_j):\n",
    "                    curr_x = curr_x_end = out_x = 0\n",
    "                    curr_y_end = curr_y + self.f_j\n",
    "                    if(curr_y_end > imag_j):\n",
    "                        curr_y_end = imag_j\n",
    "                        f_y_end = curr_y_end - curr_y\n",
    "                    f_x_end = self.f_i\n",
    "                    while(out_x < out_i):\n",
    "                        curr_x_end = curr_x + self.f_i\n",
    "                        if(curr_x_end > imag_i):\n",
    "                            curr_x_end = imag_i\n",
    "                            f_x_end = curr_x_end - curr_x\n",
    "                        z[i,curr_f,out_x,out_y] = np.sum(image[:,curr_x:curr_x_end,curr_y:curr_y_end]*self.weight[curr_f,:,0:f_x_end,0:f_y_end]) + self.bias[0][curr_f]\n",
    "                        curr_x += self.stride_i\n",
    "                        out_x += 1\n",
    "                    curr_y += self.stride_j\n",
    "                    out_y += 1\n",
    "            i+=1\n",
    "        return z,self.activation.f(z)\n",
    "    \n",
    "    def FFeedBack(self,dprev,imag_in):\n",
    "        (num_imag, num_ch, imag_i, imag_j) = imag_in.shape\n",
    "\n",
    "        if(self.num_ch != num_ch):\n",
    "            print(\"Big mistake! The number of channels in the filter does not match the number of channels in the input!\")\n",
    "\n",
    "        dout = np.zeros((num_imag,num_ch,imag_i,imag_j))\n",
    "        dfilt = np.zeros(self.weight.shape)\n",
    "        dbias = np.zeros((1,self.num_filt))\n",
    "\n",
    "        for i in range(imag_in.shape[0]):\n",
    "            for curr_f in range(self.num_filt):\n",
    "                curr_y = out_y = 0\n",
    "                f_y_end = self.f_i\n",
    "                while(curr_y + self.f_j < imag_j + 1):\n",
    "                    curr_x = out_x = 0\n",
    "                    curr_y_end = curr_y + self.f_j\n",
    "                    if(curr_y_end > imag_j):\n",
    "                        curr_y_end = imag_j\n",
    "                        f_y_end = curr_y_end - curr_y + 1\n",
    "                    f_x_end = self.f_j\n",
    "                    while(curr_x + self.f_i < imag_i + 1): \n",
    "                        curr_x_end = curr_x + self.f_i\n",
    "                        if(curr_x_end > imag_i):\n",
    "                            curr_x_end = imag_i\n",
    "                            f_x_end = curr_x_end - curr_x + 1                      \n",
    "                        dout[i,:,curr_x:curr_x_end,curr_y:curr_y_end] += dprev[i,curr_f, out_x, out_y] * self.weight[curr_f,:,:f_x_end,:f_y_end]\n",
    "                        dfilt[curr_f,:,:f_x_end,:f_y_end] += dprev[i,curr_f,out_x,out_y] * imag_in[i,:,curr_x:curr_x_end,curr_y:curr_y_end]\n",
    "\n",
    "                        curr_x += self.stride_i\n",
    "                        out_x +=1\n",
    "                    curr_y += self.stride_j\n",
    "                    out_y += 1\n",
    "                dbias[0][curr_f] += np.sum(dprev[i,curr_f])\n",
    "        '''\n",
    "        print('')\n",
    "        self.FPrint()\n",
    "        print('dprev\\n',dprev)\n",
    "        print('imag_in\\n',imag_in) \n",
    "        print('dfilt\\n',dfilt)\n",
    "        print('dbias\\n',dbias)\n",
    "        '''\n",
    "        return dout,dfilt,dbias\n",
    "\n",
    "    def FPrint(self):\n",
    "        print(\"Filter Information:\")\n",
    "        print(\"  tag:\",self.tag)\n",
    "        print(\"  num_filt:\",self.num_filt)\n",
    "        print(\"  num_ch:\",self.num_ch)\n",
    "        print(\"  f_i:\",self.f_i)\n",
    "        print(\"  f_j:\",self.f_j)\n",
    "        print(\"  stride_i:\",self.stride_i)\n",
    "        print(\"  stride_j:\",self.stride_j)\n",
    "        #print(\"  act.f:\",self.activation.f)\n",
    "        #print(\"  act.d:\",self.activation.d)\n",
    "        print(\"  filt:\\n\",self.weight)\n",
    "        print(\"  bias:\\n\",self.bias)\n",
    "\n",
    "class CPool:\n",
    "    def __init__(self,info):\n",
    "        self.tag = \"pool\"\n",
    "        self.dimensions = info.dimensions\n",
    "        self.num_ch = info.dimensions[0]\n",
    "        self.pool_i = info.dimensions[1]\n",
    "        self.pool_j = info.dimensions[2]\n",
    "        self.stride_i = info.stride_i\n",
    "        self.stride_j = info.stride_j  \n",
    "        \n",
    "    def FFeedForward(self,images):\n",
    "        (num_imag, num_ch, imag_i, imag_j) = images.shape\n",
    "\n",
    "        if(self.num_ch != num_ch):\n",
    "            print(\"Big mistake! The number of channels in the pool does not match the number of channels in the input!\")\n",
    "\n",
    "        out_i = int(np.ceil((imag_i - self.pool_i)/self.stride_i)) + 1\n",
    "        out_j = int(np.ceil((imag_j - self.pool_j)/self.stride_j)) + 1\n",
    "        z = np.zeros((num_imag, num_ch, out_i, out_j))\n",
    "\n",
    "        i=0\n",
    "        for image in images:\n",
    "            for curr_chan in range(self.num_ch):\n",
    "                curr_y = out_y = 0\n",
    "                while(out_y < out_j):\n",
    "                    curr_x = out_x = 0\n",
    "                    curr_y_end = curr_y + self.pool_j\n",
    "                    if(curr_y_end > imag_j):\n",
    "                        current_y_end = imag_j\n",
    "                    while(out_x < out_i):\n",
    "                        curr_x_end = curr_x + self.pool_i\n",
    "                        if(curr_x_end > imag_i):\n",
    "                            curr_x_end = imag_i\n",
    "                        z[i,curr_chan,out_x,out_y] = np.max(image[curr_chan, curr_x:curr_x_end, curr_y:curr_y_end])\n",
    "                        curr_x += self.stride_i\n",
    "                        out_x +=1\n",
    "                    curr_y += self.stride_j\n",
    "                    out_y += 1\n",
    "            i+=1\n",
    "        return z,z\n",
    "\n",
    "    def FFeedBack(self, dprev, imag_in):\n",
    "        (num_imag_p, num_ch_p, dp_i, dp_j) = dprev.shape\n",
    "        (num_imag, num_ch, imag_i, imag_j) = imag_in.shape\n",
    "\n",
    "        if(num_ch_p != num_ch):\n",
    "            print(\"Big mistake! The number of channels in dpool does not match the number of channels in the input!\")\n",
    "\n",
    "        dout = np.zeros((num_imag, num_ch,imag_i,imag_j))\n",
    "        for i in range(imag_in.shape[0]):\n",
    "            for curr_ch in range(num_ch):\n",
    "                curr_y = curr_y_end = out_y = 0\n",
    "                while(out_y < dp_j):\n",
    "                    curr_x = curr_x_end = out_x = 0\n",
    "                    curr_y_end = curr_y + self.pool_j\n",
    "                    if(curr_y_end > imag_j):\n",
    "                        curr_y_end = imag_j\n",
    "                    while(out_x < dp_i):\n",
    "                        curr_x_end = curr_x + self.pool_i\n",
    "                        if(curr_x_end > imag_i):\n",
    "                            curr_x_end = imag_i\n",
    "                        (a,b) = np.unravel_index(np.nanargmax(imag_in[i,curr_ch,curr_x:curr_x_end,curr_y:curr_y_end]),imag_in[i,curr_ch,curr_x:curr_x_end,curr_y:curr_y_end].shape)\n",
    "                        dout[i,curr_ch,curr_x+a,curr_y+b] += dprev[i,curr_ch,out_x,out_y]\n",
    "\n",
    "                        curr_x += self.stride_i\n",
    "                        out_x += 1\n",
    "                    curr_y += self.stride_j\n",
    "                    out_y += 1\n",
    "        '''\n",
    "        print('')\n",
    "        self.FPrint()\n",
    "        print('dprev\\n',dprev)\n",
    "        print('imag_in\\n',imag_in)\n",
    "        '''\n",
    "        return dout\n",
    "\n",
    "    def FPrint(self):\n",
    "        print(\"Pool Information:\")\n",
    "        print(\"  tag:\",self.tag)\n",
    "        print(\"  pool_i:\",self.pool_i)\n",
    "        print(\"  pool_j:\",self.pool_j)\n",
    "        print(\"  stide_i:\",self.stride_i)\n",
    "        print(\"  stride_j:\",self.stride_j)\n",
    "\n",
    "class CConnection:\n",
    "    def __init__(self,info):\n",
    "        self.tag = \"connection\"\n",
    "        self.dimensions = info.dimensions\n",
    "        self.neurons_in = info.dimensions[0]\n",
    "        self.neurons_out = info.dimensions[1]\n",
    "        self.weight = self.FInitializeWeights()\n",
    "        self.bias = self.FInitializeBias()\n",
    "        self.activation = info.activation\n",
    "   \n",
    "    def FInitializeWeights(self):\n",
    "        return np.random.normal(loc=0,scale=1.0/(np.sqrt(np.prod([self.neurons_in,self.neurons_out]))),size=[self.neurons_in,self.neurons_out])\n",
    "       \n",
    "    def FInitializeBias(self):\n",
    "        return np.random.normal(loc=0,scale=1.0/(np.sqrt(np.prod(self.neurons_out))),size=(1,self.neurons_out))\n",
    "\n",
    "    def FFeedForward(self,images):\n",
    "        z = images.dot(self.weight) + self.bias\n",
    "        return z, self.activation.f(z)\n",
    "    \n",
    "    def FFeedBack(self,dprev,data_in):\n",
    "        dW = self.activation.f(data_in).T.dot(dprev)\n",
    "        db = np.sum(dprev,axis=0,keepdims=True)\n",
    "        dout = dprev.dot(self.weight.T)*self.activation.d(data_in)\n",
    "        '''\n",
    "        print('')\n",
    "        self.FPrint()\n",
    "        print('dprev\\n',dprev)\n",
    "        print('f(data)\\n',self.activation.f(data_in))\n",
    "        print('d(data)\\n',self.activation.d(data_in))\n",
    "        print('weight\\n',self.weight)\n",
    "        print('bias\\n',self.bias)\n",
    "        print('dW\\n',dW)\n",
    "        print('db\\n',db)\n",
    "        '''\n",
    "        return dout, dW, db\n",
    "\n",
    "    def FPrint(self):\n",
    "        print(\"Connection Information:\")\n",
    "        print(\"  tag:\",self.tag)\n",
    "        print(\"  neurons_in:\",self.neurons_in)\n",
    "        print(\"  neurons_out:\",self.neurons_out)\n",
    "        #print(\"  act.f:\",self.activation.f)\n",
    "        #print(\"  act.d:\",self.activation.d)\n",
    "        print(\"  weights:\\n\",self.weight)\n",
    "        print(\"  bias:\\n\",self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLinear(X):\n",
    "    return X\n",
    "def FRELU(X):\n",
    "    a = np.copy(X)\n",
    "    a[X<0]=0\n",
    "    return a\n",
    "def FRELU_dx(X):\n",
    "    dx = np.zeros(X.shape)\n",
    "    dx[X>0] = 1\n",
    "    return dx\n",
    "\n",
    "def RegularizedLoss(y,yhat,regularization_parameter,C):\n",
    "    loss = 0\n",
    "    hinge = 1 - y*yhat\n",
    "    hinge = np.sum(hinge,axis=1,keepdims=True)\n",
    "    hinge[hinge<0]=0\n",
    "    loss = np.sum(hinge)\n",
    "    loss *= regularization_parameter\n",
    "    temp = 0\n",
    "    for item in C:\n",
    "        for elem in item:\n",
    "            temp += np.sum(elem*elem)\n",
    "    return loss + np.sqrt(temp)\n",
    "\n",
    "def RegularizedLoss_dx(y,yhat,regularization_parameter):\n",
    "    hinge = 1 - y*yhat\n",
    "    hinge = np.sum(hinge,axis=1,keepdims=True)\n",
    "    summed = np.sum(y*yhat,axis=1,keepdims=True)\n",
    "    loss = np.zeros(y.shape)\n",
    "    for i in range(hinge.shape[0]):\n",
    "        if(hinge[i]>0):\n",
    "            loss[i] = yhat[i]*(summed[i] - y[i])\n",
    "    loss *= regularization_parameter\n",
    "    return loss\n",
    "\n",
    "def softmax(z):\n",
    "    exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True)) # for stablility\n",
    "    # keepdims = True means that the output's dimension is the same as of z\n",
    "    softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "    return softmax_scores\n",
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))\n",
    "\n",
    "class CActivation:\n",
    "    def __init__(self,Function,Derivative):\n",
    "        self.f = Function\n",
    "        self.d = Derivative\n",
    "\n",
    "RELU  = CActivation(FRELU,FRELU_dx)\n",
    "class CLayer:\n",
    "    def __init__(self,Tag,Info):\n",
    "        self.tag = Tag\n",
    "        self.info = Info\n",
    "        self.indim = None\n",
    "        self.outdim = None\n",
    "        \n",
    "class CInfo:\n",
    "    def __init__(self,dimensions=None,stride_i=1,stride_j=1,activation=RELU):\n",
    "        self.dimensions = dimensions\n",
    "        self.stride_i = stride_i\n",
    "        self.stride_j = stride_j\n",
    "        self.activation = activation\n",
    "    def FAddDimensions(self,Dimensions):\n",
    "        self.dimensions = Dimensions\n",
    "    def FAddStride(self,Stride_i=1,Stride_j=1):\n",
    "        self.stride_i = Stride_i\n",
    "        self.stride_j = Stride_j\n",
    "    def FAddActivation(self,Activation):\n",
    "        self.activation = Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, indim, outdim, LayersInfo, lr=0.1, rp=0.1):\n",
    "        self.in_ch = []\n",
    "        self.indim_i = []\n",
    "        self.indim_j = []\n",
    "        self.in_ch.append(indim[0])\n",
    "        self.indim_i.append(indim[1])\n",
    "        self.indim_j.append(indim[1])\n",
    "        self.outdim = outdim\n",
    "\n",
    "        #self.X = X\n",
    "        #self.X_reshape = self.X.reshape(self.samples,self.in_ch[0],self.indim_i[0],self.indim_j[0])\n",
    "        #self.y = y\n",
    "        self.lr = lr\n",
    "        self.rp = rp\n",
    "        self.probability = softmax\n",
    "\n",
    "        self.layer = []\n",
    "        for Layer in LayersInfo:\n",
    "            self.layer.append(self.FAddLayer(Layer))\n",
    "        outinfo = CInfo([-1,self.outdim])\n",
    "        finallayer = CLayer('connection',outinfo)\n",
    "        self.layer.append(self.FAddLayer(finallayer))\n",
    "            \n",
    "\n",
    "    def FAddLayer(self,Layer):\n",
    "        if(Layer.tag=='convolution'):\n",
    "            Layer.info.dimensions[1] = self.in_ch[-1]\n",
    "            self.in_ch.append(Layer.info.dimensions[0])\n",
    "            f_i = Layer.info.dimensions[2]\n",
    "            f_j = Layer.info.dimensions[3]\n",
    "            stride_i = Layer.info.stride_i\n",
    "            stride_j = Layer.info.stride_j\n",
    "            indim_i = self.indim_i[-1]\n",
    "            indim_j = self.indim_j[-1]\n",
    "            self.indim_i.append(int(np.ceil((indim_i - f_i)/stride_i) + 1))\n",
    "            self.indim_j.append(int(np.ceil((indim_j - f_j)/stride_j) + 1))\n",
    "            return CFilter(Layer.info)\n",
    "        elif(Layer.tag=='pool'):\n",
    "            Layer.info.dimensions[0] = self.in_ch[-1]\n",
    "            p_i = Layer.info.dimensions[1]\n",
    "            p_j = Layer.info.dimensions[2]\n",
    "            stride_i = Layer.info.stride_i\n",
    "            stride_j = Layer.info.stride_j\n",
    "            indim_i = self.indim_i[-1]\n",
    "            indim_j = self.indim_j[-1]\n",
    "            self.in_ch.append(self.in_ch[-1])\n",
    "            self.indim_i.append(int(np.ceil((indim_i - p_i)/stride_i) + 1))\n",
    "            self.indim_j.append(int(np.ceil((indim_j - p_j)/stride_j) + 1))\n",
    "            return CPool(Layer.info)\n",
    "        elif(Layer.tag=='connection'):\n",
    "            flatten = self.in_ch[-1]*self.indim_i[-1]*self.indim_j[-1]\n",
    "            Layer.info.dimensions[0] = flatten\n",
    "            self.in_ch.append(1)\n",
    "            self.indim_i.append(1)\n",
    "            self.indim_j.append(Layer.info.dimensions[1])\n",
    "            return CConnection(Layer.info)\n",
    "        else:\n",
    "            print(Layer.tag,'is not a valid layer option. Double check initialization.')\n",
    "            return None\n",
    "\n",
    "    def FPrint(self):\n",
    "        for l in self.layer:\n",
    "            l.FPrint()\n",
    "\n",
    "    def FFeedForward(self,X):\n",
    "        self.z = []\n",
    "        self.f = []\n",
    "        images = X.reshape(X.shape[0],self.in_ch[0],self.indim_i[0],self.indim_j[0])\n",
    "        self.f.append(images)\n",
    "        for i in range(0,len(self.layer)):\n",
    "            if(self.layer[i].tag == 'connection'):\n",
    "                images = np.reshape(images,(images.shape[0],-1))\n",
    "            Z,F = self.layer[i].FFeedForward(images)\n",
    "            self.z.append(Z)\n",
    "            if(i != len(self.layer) -1):\n",
    "                self.f.append(F)\n",
    "            images = self.f[-1]\n",
    "            '''\n",
    "            print('')\n",
    "            print(self.layer[i].FPrint())\n",
    "            print('z',Z)\n",
    "            print('f',F)\n",
    "            '''\n",
    "        self.yhat = self.probability(self.z[-1])\n",
    "        \n",
    "    def FBackPropagation(self,y):\n",
    "        d = []\n",
    "        d.insert(0,self.Loss_dx(y,self.yhat)) \n",
    "        #print('dout\\n',d[0])\n",
    "        for i in range(len(self.layer)):\n",
    "            images = self.f[-1-i]\n",
    "            if(self.layer[-1-i].tag == 'connection'):\n",
    "                images = np.reshape(images,(images.shape[0],-1))\n",
    "            elif(self.layer[-1-i+1].tag == 'connection'):\n",
    "                d[0] = np.reshape(d[0],(images.shape[0],self.in_ch[-1-i],self.indim_i[-1-i],self.indim_j[-1-i]))\n",
    "            if(self.layer[-1-i].tag != 'pool'):\n",
    "                dout,dW,db = self.layer[-1-i].FFeedBack(d[0],images)\n",
    "                #dW /= float(len(y))\n",
    "                #db /= float(len(y))\n",
    "                self.layer[-1-i].weight = self.layer[-1-i].weight - self.lr*dW\n",
    "                if(self.rp != 0 and self.layer[-1-i].weight.all() !=0 ):\n",
    "                    self.layer[-1-i].weight = self.layer[-1-i].weight - self.rp*self.layer[-1-i].weight/np.sqrt(np.sum(self.layer[-1-i].weight*self.layer[-1-i].weight))/self.layer[-1-i].weight.size\n",
    "\n",
    "                self.layer[-1-i].bias = self.layer[-1-i].bias - self.lr*db\n",
    "                if(self.rp != 0 and self.layer[-1-i].bias.all() !=0 ):\n",
    "                    self.layer[-1-i].bias = self.layer[-1-i].bias - self.rp*self.layer[-1-i].bias/np.sqrt(np.sum(self.layer[-1-i].bias*self.layer[-1-i].bias))/self.layer[-1-i].bias.size  \n",
    "            else:\n",
    "                dout = self.layer[-1-i].FFeedBack(d[0],self.f[-1-i])\n",
    "                \n",
    "            d.insert(0,dout)\n",
    "            #print('dout\\n',d[0])\n",
    "            \n",
    "    def Loss(self,y,yhat):\n",
    "        loss = 0\n",
    "        hinge = 1 - y*yhat\n",
    "        hinge = np.sum(hinge,axis=1,keepdims=True)\n",
    "        hinge[hinge<0]=0\n",
    "        loss = np.sum(hinge)\n",
    "        loss *= self.rp\n",
    "        temp = 0\n",
    "        for l in self.layer:\n",
    "            if(l.tag != 'pool'):\n",
    "                temp += np.sum(l.weight*l.weight)\n",
    "                temp += np.sum(l.bias*l.bias)\n",
    "        return loss + np.sqrt(temp)\n",
    "\n",
    "    def Loss_dx(self,y,yhat):\n",
    "        hinge = 1 - y*yhat\n",
    "        #print('hinge\\n',hinge)\n",
    "        hinge = np.sum(hinge,axis=1,keepdims=True)\n",
    "        #print('hinge\\n',hinge)\n",
    "        summed = np.sum(y*yhat,axis=1,keepdims=True)\n",
    "        #print('summed\\n',summed)\n",
    "        loss = np.zeros(y.shape)\n",
    "        for i in range(hinge.shape[0]):\n",
    "            if(hinge[i]>0):\n",
    "                loss[i] = yhat[i]*(summed[i] - y[i])\n",
    "        #print('dloss\\n',loss)\n",
    "        loss *= self.rp\n",
    "        #print('dloss\\n',loss)\n",
    "        return loss   \n",
    "    \n",
    "    def FPredict(self, X_test):\n",
    "        images = X_test.reshape(X_test.shape[0],self.in_ch[0],self.indim_i[0],self.indim_j[0])\n",
    "        f = images\n",
    "        for i in range(0,len(self.layer)):\n",
    "            if(self.layer[i].tag == 'connection'):\n",
    "                images = np.reshape(images,(images.shape[0],-1))\n",
    "            Z,F = self.layer[i].FFeedForward(images)\n",
    "            images = F\n",
    "        yhat = self.probability(Z)\n",
    "       \n",
    "        # the rest is similar to the logistic regression\n",
    "        labels = np.arange(0,self.outdim+1)\n",
    "        num_test_samples = X_test.shape[0]\n",
    "        # find which index gives us the highest probability\n",
    "        ypred = np.zeros(num_test_samples, dtype=int)\n",
    "        for i in range(num_test_samples):\n",
    "            ypred[i] = labels[np.argmax(yhat[i,:])]\n",
    "        return ypred\n",
    "\n",
    "    def FTrain(self,epochs,batchsize,X,y,Xtest,ytest):\n",
    "        batches = int(np.ceil(X.shape[0]/batchsize))\n",
    "        for i in range(epochs):\n",
    "            for j in range(batches):\n",
    "                start = j*batchsize\n",
    "                end = start + batchsize\n",
    "                if(end > X_train_norm.shape[0]):\n",
    "                    end = X_train_norm.shape[0]\n",
    "                X_ = X[start:end]\n",
    "                y_ = y[start:end]\n",
    "                cnn.FFeedForward(X_)\n",
    "                cnn.FBackPropagation(y_)\n",
    "                #if((i+1)%(epochs/10) == 0):\n",
    "                ypred = cnn.FPredict(Xtest)\n",
    "                print(i+1,j,accuracy(ypred.ravel(),ytest.ravel()),'\\n  Loss: ',self.Loss(y_,self.yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0.18444444444444444 \n",
      "Loss:  135.96337930365544\n",
      "1 1 0.13333333333333333 \n",
      "Loss:  135.97232038783682\n",
      "1 2 0.13777777777777778 \n",
      "Loss:  135.99128822512927\n",
      "1 3 0.19555555555555557 \n",
      "Loss:  135.94999337664555\n",
      "1 4 0.2088888888888889 \n",
      "Loss:  135.9588073844459\n",
      "1 5 0.1711111111111111 \n",
      "Loss:  135.9545311382693\n",
      "1 6 0.20666666666666667 \n",
      "Loss:  135.9509844867974\n",
      "1 7 0.2111111111111111 \n",
      "Loss:  135.92385824435794\n",
      "1 8 0.21333333333333335 \n",
      "Loss:  135.94726506776976\n",
      "1 9 0.21333333333333335 \n",
      "Loss:  132.92502003807977\n",
      "2 0 0.21333333333333335 \n",
      "Loss:  135.8795078043894\n",
      "2 1 0.20666666666666667 \n",
      "Loss:  135.88617893159076\n",
      "2 2 0.20666666666666667 \n",
      "Loss:  135.89992800292026\n",
      "2 3 0.21333333333333335 \n",
      "Loss:  135.86972002237476\n",
      "2 4 0.21555555555555556 \n",
      "Loss:  135.87663739575225\n",
      "2 5 0.21333333333333335 \n",
      "Loss:  135.8735806296076\n",
      "2 6 0.21333333333333335 \n",
      "Loss:  135.87205230339515\n",
      "2 7 0.21555555555555556 \n",
      "Loss:  135.85205233783543\n",
      "2 8 0.21555555555555556 \n",
      "Loss:  135.86757243491033\n",
      "2 9 0.21333333333333335 \n",
      "Loss:  132.8595898758506\n",
      "3 0 0.21555555555555556 \n",
      "Loss:  135.81744449568907\n",
      "3 1 0.21555555555555556 \n",
      "Loss:  135.821437575694\n",
      "3 2 0.21555555555555556 \n",
      "Loss:  135.8284929558423\n",
      "3 3 0.21555555555555556 \n",
      "Loss:  135.8103219263056\n",
      "3 4 0.21555555555555556 \n",
      "Loss:  135.81519406835145\n",
      "3 5 0.21333333333333335 \n",
      "Loss:  135.8129541654748\n",
      "3 6 0.21777777777777776 \n",
      "Loss:  135.81396378332656\n",
      "3 7 0.21777777777777776 \n",
      "Loss:  135.8021295963491\n",
      "3 8 0.21777777777777776 \n",
      "Loss:  135.8093304199572\n",
      "3 9 0.21777777777777776 \n",
      "Loss:  132.8139115073819\n",
      "4 0 0.21777777777777776 \n",
      "Loss:  135.77634933867708\n",
      "4 1 0.21777777777777776 \n",
      "Loss:  135.77735437398144\n",
      "4 2 0.22 \n",
      "Loss:  135.77713221645607\n",
      "4 3 0.23333333333333334 \n",
      "Loss:  135.77205014162112\n",
      "4 4 0.26 \n",
      "Loss:  135.77594231394716\n",
      "4 5 0.34 \n",
      "Loss:  135.7737675107657\n",
      "4 6 0.38 \n",
      "Loss:  135.77713049732859\n",
      "4 7 0.3888888888888889 \n",
      "Loss:  135.7761312086825\n",
      "4 8 0.4177777777777778 \n",
      "Loss:  135.77302347802404\n",
      "4 9 0.4088888888888889 \n",
      "Loss:  132.79012326042744\n",
      "5 0 0.4088888888888889 \n",
      "Loss:  135.75845758185702\n",
      "5 1 0.4111111111111111 \n",
      "Loss:  135.7555886748957\n",
      "5 2 0.4222222222222222 \n",
      "Loss:  135.745981686556\n",
      "5 3 0.42 \n",
      "Loss:  135.75616501226335\n",
      "5 4 0.44 \n",
      "Loss:  135.76099780351132\n",
      "5 5 0.4533333333333333 \n",
      "Loss:  135.7576810416813\n",
      "5 6 0.4533333333333333 \n",
      "Loss:  135.76148640090454\n",
      "5 7 0.4533333333333333 \n",
      "Loss:  135.77664184939024\n",
      "5 8 0.4533333333333333 \n",
      "Loss:  135.7579598113706\n",
      "5 9 0.4622222222222222 \n",
      "Loss:  132.78785827148542\n",
      "6 0 0.4533333333333333 \n",
      "Loss:  135.76448955340686\n",
      "6 1 0.44666666666666666 \n",
      "Loss:  135.755955038299\n",
      "6 2 0.42444444444444446 \n",
      "Loss:  135.73157079094398\n",
      "6 3 0.43777777777777777 \n",
      "Loss:  135.76085931085984\n",
      "6 4 0.4622222222222222 \n",
      "Loss:  135.77031189284318\n",
      "6 5 0.4688888888888889 \n",
      "Loss:  135.76340250879767\n",
      "6 6 0.4488888888888889 \n",
      "Loss:  135.76317896086096\n",
      "6 7 0.46 \n",
      "Loss:  135.80475746416138\n",
      "6 8 0.44666666666666666 \n",
      "Loss:  135.76017156625522\n",
      "6 9 0.43777777777777777 \n",
      "Loss:  132.79912481336365\n",
      "7 0 0.44222222222222224 \n",
      "Loss:  135.78491186376544\n",
      "7 1 0.43777777777777777 \n",
      "Loss:  135.7687802048927\n",
      "7 2 0.3888888888888889 \n",
      "Loss:  135.71451739770868\n",
      "7 3 0.39111111111111113 \n",
      "Loss:  135.7644022133293\n",
      "7 4 0.4177777777777778 \n",
      "Loss:  135.79094179315706\n",
      "7 5 0.42444444444444446 \n",
      "Loss:  135.7714817062269\n",
      "7 6 0.3844444444444444 \n",
      "Loss:  135.74050749476467\n",
      "7 7 0.38222222222222224 \n",
      "Loss:  135.85811862909097\n",
      "7 8 0.34 \n",
      "Loss:  135.7209324490784\n",
      "7 9 0.32222222222222224 \n",
      "Loss:  132.7674528282887\n",
      "8 0 0.3111111111111111 \n",
      "Loss:  135.7773102602474\n",
      "8 1 0.30666666666666664 \n",
      "Loss:  135.71436999759356\n",
      "8 2 0.24444444444444444 \n",
      "Loss:  135.50119772987136\n",
      "8 3 0.24444444444444444 \n",
      "Loss:  135.59206683652118\n",
      "8 4 0.2688888888888889 \n",
      "Loss:  135.70444781180757\n",
      "8 5 0.29333333333333333 \n",
      "Loss:  135.55622054847046\n",
      "8 6 0.2733333333333333 \n",
      "Loss:  135.32674735051057\n",
      "8 7 0.29777777777777775 \n",
      "Loss:  135.78740781418577\n",
      "8 8 0.28444444444444444 \n",
      "Loss:  135.17148185234294\n",
      "8 9 0.2911111111111111 \n",
      "Loss:  132.26590523237988\n",
      "9 0 0.29555555555555557 \n",
      "Loss:  135.23421945294143\n",
      "9 1 0.33555555555555555 \n",
      "Loss:  135.05863073501604\n",
      "9 2 0.3377777777777778 \n",
      "Loss:  134.37239033122205\n",
      "9 3 0.39555555555555555 \n",
      "Loss:  134.72390680675636\n",
      "9 4 0.36666666666666664 \n",
      "Loss:  134.89423664883947\n",
      "9 5 0.41333333333333333 \n",
      "Loss:  134.47109576048445\n",
      "9 6 0.38222222222222224 \n",
      "Loss:  134.2396673379927\n",
      "9 7 0.3933333333333333 \n",
      "Loss:  134.97073841078506\n",
      "9 8 0.3844444444444444 \n",
      "Loss:  133.94929813741308\n",
      "9 9 0.5622222222222222 \n",
      "Loss:  131.2927934605229\n",
      "10 0 0.56 \n",
      "Loss:  133.7105986003025\n",
      "10 1 0.6 \n",
      "Loss:  133.6517934646397\n",
      "10 2 0.5711111111111111 \n",
      "Loss:  132.50478967356338\n",
      "10 3 0.5666666666666667 \n",
      "Loss:  132.74186930794733\n",
      "10 4 0.5911111111111111 \n",
      "Loss:  132.84954433502494\n",
      "10 5 0.5466666666666666 \n",
      "Loss:  132.27527909246163\n",
      "10 6 0.5977777777777777 \n",
      "Loss:  133.24627681671606\n",
      "10 7 0.6177777777777778 \n",
      "Loss:  133.26988901378016\n",
      "10 8 0.5977777777777777 \n",
      "Loss:  132.45859881586105\n",
      "10 9 0.6111111111111112 \n",
      "Loss:  129.21359740801037\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = read_dataset('Digits_X_train.csv', 'Digits_y_train.csv')\n",
    "X_test, y_test = read_dataset('Digits_X_test.csv', 'Digits_y_test.csv')\n",
    "X_train_norm, X_test_norm = normalize_features(X_train_digits, X_test_digits)\n",
    "y_train_ohe, y_test_ohe = one_hot_encoder(y_train_digits, y_test_digits)\n",
    "\n",
    "\n",
    "filt_info = CInfo([8,-1,3,3])\n",
    "filt = CLayer('convolution',filt_info)\n",
    "\n",
    "pool_info = CInfo([-1,2,2],2,2)\n",
    "pool = CLayer('pool',pool_info)\n",
    "\n",
    "conn_info = CInfo([-1,100])\n",
    "conn = CLayer('connection',conn_info)\n",
    "\n",
    "cnn = CNN([1,8,8],10,[filt,pool,conn],0.05,0.1)\n",
    "epochs = 10\n",
    "batches = 10\n",
    "batchsize = int (np.ceil(X_train_norm.shape[0]/batches))\n",
    "cnn.FTrain(epochs,\\\n",
    "           batchsize,\\\n",
    "           X_train_norm,\\\n",
    "           y_train_ohe,\\\n",
    "           X_test_norm,\\\n",
    "           y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X\n",
      " [[[[-16. -15. -14. -13.]\n",
      "   [-12. -11. -10.  -9.]\n",
      "   [ -8.  -7.  -6.  -5.]\n",
      "   [ -4.  -3.  -2.  -1.]]]\n",
      "\n",
      "\n",
      " [[[  0.   1.   2.   3.]\n",
      "   [  4.  10.   6.   7.]\n",
      "   [  8.   9.  10.  11.]\n",
      "   [ 12.  13.  14.  15.]]]]\n",
      "\n",
      "y\n",
      " [[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "yhat\n",
      " [[ 0.  1. -1.]\n",
      " [ 0.  1. -1.]]\n",
      "\n",
      "Filter Information:\n",
      "  tag: convolution\n",
      "  num_filt: 2\n",
      "  num_ch: 1\n",
      "  f_i: 2\n",
      "  f_j: 2\n",
      "  stride_i: 1\n",
      "  stride_j: 1\n",
      "  filt:\n",
      " [[[[0 1]\n",
      "   [2 3]]]\n",
      "\n",
      "\n",
      " [[[4 5]\n",
      "   [6 7]]]]\n",
      "  bias:\n",
      " [[0 1]]\n",
      "None\n",
      "z [[[[ -72.  -66.  -60.]\n",
      "   [ -48.  -42.  -36.]\n",
      "   [ -24.  -18.  -12.]]\n",
      "\n",
      "  [[-287. -265. -243.]\n",
      "   [-199. -177. -155.]\n",
      "   [-111.  -89.  -67.]]]\n",
      "\n",
      "\n",
      " [[[  39.   40.   36.]\n",
      "   [  53.   54.   60.]\n",
      "   [  72.   78.   84.]]\n",
      "\n",
      "  [[ 100.  117.  109.]\n",
      "   [ 178.  195.  197.]\n",
      "   [ 241.  263.  285.]]]]\n",
      "f [[[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[ 39.  40.  36.]\n",
      "   [ 53.  54.  60.]\n",
      "   [ 72.  78.  84.]]\n",
      "\n",
      "  [[100. 117. 109.]\n",
      "   [178. 195. 197.]\n",
      "   [241. 263. 285.]]]]\n",
      "\n",
      "Pool Information:\n",
      "  tag: pool\n",
      "  pool_i: 2\n",
      "  pool_j: 2\n",
      "  stide_i: 2\n",
      "  stride_j: 2\n",
      "None\n",
      "z [[[[  0.   0.]\n",
      "   [  0.   0.]]\n",
      "\n",
      "  [[  0.   0.]\n",
      "   [  0.   0.]]]\n",
      "\n",
      "\n",
      " [[[ 54.  60.]\n",
      "   [ 78.  84.]]\n",
      "\n",
      "  [[195. 197.]\n",
      "   [263. 285.]]]]\n",
      "f [[[[  0.   0.]\n",
      "   [  0.   0.]]\n",
      "\n",
      "  [[  0.   0.]\n",
      "   [  0.   0.]]]\n",
      "\n",
      "\n",
      " [[[ 54.  60.]\n",
      "   [ 78.  84.]]\n",
      "\n",
      "  [[195. 197.]\n",
      "   [263. 285.]]]]\n",
      "\n",
      "Connection Information:\n",
      "  tag: connection\n",
      "  neurons_in: 8\n",
      "  neurons_out: 2\n",
      "  weights:\n",
      " [[ 0.  1.]\n",
      " [ 2.  3.]\n",
      " [ 4.  5.]\n",
      " [ 6.  7.]\n",
      " [ 8.  9.]\n",
      " [10. 11.]\n",
      " [12. 13.]\n",
      " [14. 15.]]\n",
      "  bias:\n",
      " [[0. 1.]]\n",
      "None\n",
      "z [[0.0000e+00 1.0000e+00]\n",
      " [1.1612e+04 1.2829e+04]]\n",
      "f [[0.0000e+00 1.0000e+00]\n",
      " [1.1612e+04 1.2829e+04]]\n",
      "\n",
      "Connection Information:\n",
      "  tag: connection\n",
      "  neurons_in: 2\n",
      "  neurons_out: 3\n",
      "  weights:\n",
      " [[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "  bias:\n",
      " [[0. 1. 2.]]\n",
      "None\n",
      "z [[3.0000e+00 5.0000e+00 7.0000e+00]\n",
      " [3.8487e+04 6.2929e+04 8.7371e+04]]\n",
      "f [[3.0000e+00 5.0000e+00 7.0000e+00]\n",
      " [3.8487e+04 6.2929e+04 8.7371e+04]]\n",
      "\n",
      "y\n",
      " [[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "yhat\n",
      " [[ 0.  1. -1.]\n",
      " [ 0.  1. -1.]]\n",
      "y-yhat\n",
      " [[0. 0. 3.]\n",
      " [3. 3. 6.]]\n",
      "\n",
      "dout\n",
      " [[-0. -2.  3.]\n",
      " [-0. -5.  6.]]\n",
      "\n",
      "Connection Information:\n",
      "  tag: connection\n",
      "  neurons_in: 2\n",
      "  neurons_out: 3\n",
      "  weights:\n",
      " [[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "  bias:\n",
      " [[0. 1. 2.]]\n",
      "dprev\n",
      " [[-0. -2.  3.]\n",
      " [-0. -5.  6.]]\n",
      "f(data)\n",
      " [[0.0000e+00 1.0000e+00]\n",
      " [1.1612e+04 1.2829e+04]]\n",
      "d(data)\n",
      " [[0. 1.]\n",
      " [1. 1.]]\n",
      "weight\n",
      " [[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "bias\n",
      " [[0. 1. 2.]]\n",
      "dW\n",
      " [[     0. -58060.  69672.]\n",
      " [     0. -64147.  76977.]]\n",
      "db\n",
      " [[ 0. -7.  9.]]\n",
      "dout\n",
      " [[ 0.  7.]\n",
      " [ 7. 10.]]\n",
      "\n",
      "Connection Information:\n",
      "  tag: connection\n",
      "  neurons_in: 8\n",
      "  neurons_out: 2\n",
      "  weights:\n",
      " [[ 0.  1.]\n",
      " [ 2.  3.]\n",
      " [ 4.  5.]\n",
      " [ 6.  7.]\n",
      " [ 8.  9.]\n",
      " [10. 11.]\n",
      " [12. 13.]\n",
      " [14. 15.]]\n",
      "  bias:\n",
      " [[0. 1.]]\n",
      "dprev\n",
      " [[ 0.  7.]\n",
      " [ 7. 10.]]\n",
      "f(data)\n",
      " [[  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 54.  60.  78.  84. 195. 197. 263. 285.]]\n",
      "d(data)\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "weight\n",
      " [[ 0.  1.]\n",
      " [ 2.  3.]\n",
      " [ 4.  5.]\n",
      " [ 6.  7.]\n",
      " [ 8.  9.]\n",
      " [10. 11.]\n",
      " [12. 13.]\n",
      " [14. 15.]]\n",
      "bias\n",
      " [[0. 1.]]\n",
      "dW\n",
      " [[ 378.  540.]\n",
      " [ 420.  600.]\n",
      " [ 546.  780.]\n",
      " [ 588.  840.]\n",
      " [1365. 1950.]\n",
      " [1379. 1970.]\n",
      " [1841. 2630.]\n",
      " [1995. 2850.]]\n",
      "db\n",
      " [[ 7. 17.]]\n",
      "dout\n",
      " [[  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 10.  44.  78. 112. 146. 180. 214. 248.]]\n",
      "\n",
      "Pool Information:\n",
      "  tag: pool\n",
      "  pool_i: 2\n",
      "  pool_j: 2\n",
      "  stide_i: 2\n",
      "  stride_j: 2\n",
      "dprev\n",
      " [[[[  0.   0.]\n",
      "   [  0.   0.]]\n",
      "\n",
      "  [[  0.   0.]\n",
      "   [  0.   0.]]]\n",
      "\n",
      "\n",
      " [[[ 10.  44.]\n",
      "   [ 78. 112.]]\n",
      "\n",
      "  [[146. 180.]\n",
      "   [214. 248.]]]]\n",
      "imag_in\n",
      " [[[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[ 39.  40.  36.]\n",
      "   [ 53.  54.  60.]\n",
      "   [ 72.  78.  84.]]\n",
      "\n",
      "  [[100. 117. 109.]\n",
      "   [178. 195. 197.]\n",
      "   [241. 263. 285.]]]]\n",
      "dout\n",
      " [[[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.  10.  44.]\n",
      "   [  0.  78. 112.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0. 146. 180.]\n",
      "   [  0. 214. 248.]]]]\n",
      "\n",
      "Filter Information:\n",
      "  tag: convolution\n",
      "  num_filt: 2\n",
      "  num_ch: 1\n",
      "  f_i: 2\n",
      "  f_j: 2\n",
      "  stride_i: 1\n",
      "  stride_j: 1\n",
      "  filt:\n",
      " [[[[0 1]\n",
      "   [2 3]]]\n",
      "\n",
      "\n",
      " [[[4 5]\n",
      "   [6 7]]]]\n",
      "  bias:\n",
      " [[0 1]]\n",
      "dprev\n",
      " [[[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.  10.  44.]\n",
      "   [  0.  78. 112.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0. 146. 180.]\n",
      "   [  0. 214. 248.]]]]\n",
      "imag_in\n",
      " [[[[-16. -15. -14. -13.]\n",
      "   [-12. -11. -10.  -9.]\n",
      "   [ -8.  -7.  -6.  -5.]\n",
      "   [ -4.  -3.  -2.  -1.]]]\n",
      "\n",
      "\n",
      " [[[  0.   1.   2.   3.]\n",
      "   [  4.  10.   6.   7.]\n",
      "   [  8.   9.  10.  11.]\n",
      "   [ 12.  13.  14.  15.]]]]\n",
      "dfilt\n",
      " [[[[ 2186.  2380.]\n",
      "   [ 3112.  3356.]]]\n",
      "\n",
      "\n",
      " [[[ 6946.  7004.]\n",
      "   [ 9368. 10156.]]]]\n",
      "dbias\n",
      " [[244. 788.]]\n",
      "dout\n",
      " [[[[   0.    0.    0.    0.]\n",
      "   [   0.    0.    0.    0.]\n",
      "   [   0.    0.    0.    0.]\n",
      "   [   0.    0.    0.    0.]]]\n",
      "\n",
      "\n",
      " [[[   0.    0.    0.    0.]\n",
      "   [   0.  584. 1460.  944.]\n",
      "   [   0. 1752. 4360. 2744.]\n",
      "   [   0. 1440. 3444. 2072.]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filt_info = CInfo([2,-1,2,2])\n",
    "filt = CLayer('convolution',filt_info)\n",
    "\n",
    "pool_info = CInfo([-1,2,2],2,2)\n",
    "pool = CLayer('pool',pool_info)\n",
    "\n",
    "conn_info = CInfo([-1,2])\n",
    "conn = CLayer('connection',conn_info)\n",
    "\n",
    "cnn = CNN([1,4,4],3,[filt,pool,conn],1,1)\n",
    "\n",
    "cnn.layer[0].weight = np.arange(8).reshape(2,1,2,2)\n",
    "cnn.layer[0].bias = np.arange(2).reshape(1,2)\n",
    "cnn.layer[2].weight = np.arange(16).reshape(8,2).astype(float)\n",
    "cnn.layer[2].bias = np.arange(2).reshape(1,2).astype(float)\n",
    "cnn.layer[3].weight = np.arange(6).reshape(2,3).astype(float)\n",
    "cnn.layer[3].bias = np.arange(3).reshape(1,3).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "X = np.arange(-16,16).reshape(2,1,4,4).astype(float)\n",
    "X[1,0,1,1] = 10\n",
    "y = np.arange(6).reshape(2,3).astype(float)\n",
    "yhat = np.zeros(6).reshape(2,3).astype(float)\n",
    "yhat[0][1] = 1\n",
    "yhat[0][2] = -1\n",
    "yhat[1][1] = 1\n",
    "yhat[1][2] = -1\n",
    "\n",
    "print('')\n",
    "print('X\\n',X)\n",
    "print('')\n",
    "print('y\\n',y)\n",
    "print('yhat\\n',yhat)\n",
    "cnn.FFeedForward(X)\n",
    "print('')\n",
    "print('y\\n',y)\n",
    "print('yhat\\n',yhat)\n",
    "print('y-yhat\\n',y-yhat)\n",
    "print('')\n",
    "cnn.FBackPropagation(y,yhat)\n",
    "#cnn.FPrint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\ndef InitializeFilter(dimensions):\\n    return np.random.normal(loc=0,scale=1.0/(np.sqrt(np.prod(dimensions))),size=dimensions)\\n\\n\\ndef Convolution(images, filt, bias, activat, stride_i=1,stride_j=1):\\n    (num_filt, num_ch_f, f_i, f_j) = filt.shape\\n    (num_imag, num_ch, imag_i, imag_j) = images.shape\\n    \\n    if(num_ch_f != num_ch):\\n        print(\"Big mistake! The number of channels in the filter does not match the number of channels in the images!\")\\n\\n    out_i = int(np.ceil((imag_i - f_i)/stride_i) + 1)\\n    out_j = int(np.ceil((imag_j - f_j)/stride_j) + 1)\\n    z = np.zeros((num_imag, num_filt, out_i, out_j))\\n    i=0\\n    for image in images:\\n        for curr_f in range(num_filt):\\n            curr_y = curr_y_end = out_y = 0\\n            f_y_end = f_j + 1\\n            while(out_y < out_j):\\n                curr_x = curr_x_end = out_x = 0\\n                curr_y_end = curr_y + f_j\\n                if(curr_y_end > imag_j):\\n                    curr_y_end = imag_j\\n                    f_y_end = curr_y_end - curr_y\\n                f_x_end = f_i + 1\\n                while(out_x < out_i):\\n                    curr_x_end = curr_x + f_i\\n                    if(curr_x_end > imag_i):\\n                        curr_x_end = imag_i\\n                        f_x_end = curr_x_end - curr_x\\n                    z[i,curr_f,out_x,out_y] = np.sum(image[:,curr_x:curr_x_end,curr_y:curr_y_end]*filt[curr_f,:,0:f_x_end,0:f_y_end]) + bias[curr_f]\\n                    curr_x += stride_i\\n                    out_x += 1\\n                curr_y += stride_j\\n                out_y += 1\\n        i+=1\\n    return z,activat(z)\\n\\ndef FMaxPool(images,pool_i=2,pool_j=2,stride_i=2,stride_j=2):\\n    (num_imag, num_ch, imag_i, imag_j) = images.shape\\n\\n    out_i = int(np.ceil((imag_i - pool_i)/stride_i)) + 1\\n    out_j = int(np.ceil((imag_j - pool_j)/stride_j)) + 1\\n    z = np.zeros((num_imag, num_ch, out_i, out_j))\\n    index = np.zeros((num_imag, num_ch, out_i, out_j))\\n    \\n    i=0\\n    for image in images:\\n        for curr_chan in range(num_ch):\\n            curr_y = out_y = 0\\n            while(out_y < out_j):\\n                curr_x = out_x = 0\\n                curr_y_end = curr_y + pool_j\\n                if(curr_y_end > imag_j):\\n                    current_y_end = imag_j\\n                while(out_x < out_i):\\n                    curr_x_end = curr_x + pool_i\\n                    if(curr_x_end > imag_i):\\n                        curr_x_end = imag_i\\n                    z[i,curr_chan,out_x,out_y] = np.max(image[curr_chan,curr_x:curr_x_end,curr_y:curr_y_end])\\n                    index[i,curr_chan,out_x,out_y] = np.argmax(image[curr_chan,curr_x:curr_x_end,curr_y:curr_y_end])\\n                    curr_x += stride_i\\n                    out_x +=1\\n                curr_y += stride_j\\n                out_y += 1\\n        i+=1\\n    return z,index\\n\\ndef FConnection(images,weights,bias,activation):\\n    z = images.dot(weights) + bias\\n    return z, activation(z)\\n\\ndef FbackConvolution(dprev,conv_in,filt,derivative,stride_i=1,stride_j=1):\\n    (num_filt, num_ch_f, f_i, f_j) = filt.shape\\n    (num_imag, num_ch, imag_i, imag_j) = conv_in.shape\\n    \\n    if(num_ch_f != num_ch):\\n        print(\"Big mistake! The number of channels in the filter does not match the number of channels in the input!\")\\n\\n    dout = np.zeros((num_ch,imag_i,imag_j))\\n    dfilt = np.zeros(filt.shape)\\n    dbias = np.zeros((num_filt,1))\\n    \\n    for conv in conv_in:\\n        dout_conv = np.zeros((num_ch,imag_i,imag_j))\\n        for curr_f in range(num_filt):\\n            curr_y = out_y = 0\\n            f_y_end = f_i\\n            while(curr_y + f_j < imag_j + 1):\\n                curr_x = out_x = 0\\n                curr_y_end = curr_y + f_j\\n                if(curr_y_end > imag_j):\\n                    curr_y_end = imag_j\\n                    f_y_end = curr_y_end - curr_y\\n                f_x_end = f_j\\n                while(curr_x + f_i < imag_i + 1): \\n                    curr_x_end = curr_x + f_i\\n                    if(curr_x_end > imag_i):\\n                        curr_x_end = imag_i\\n                        f_x_end = curr_x_end - curr_i\\n                    dout_conv[:,curr_x:curr_x_end,curr_y:curr_y_end] += dprev[curr_f, out_x, out_y] * filt[curr_f,:,:f_x_end,:f_y_end]\\n                    dfilt[curr_f,:,:f_x_end,:f_y_end] += dprev[curr_f,out_x,out_y] * conv[:,curr_x:curr_x_end,curr_y:curr_y_end]\\n\\n                    curr_x += stride_i\\n                    out_x +=1\\n                curr_y += stride_j\\n                out_y += 1\\n            dbias[curr_f] = np.sum(dprev[curr_f])\\n        dout += dout_conv*derivative(conv)\\n\\n    return dout,dfilt,dbias\\n\\ndef FbackMaxPool(dpool, conv_in, p_i=2, p_j=2, stride_i=2, stride_j=2):\\n    (num_ch_p, dp_i, dp_j) = dpool.shape\\n    (num_imag, num_ch, imag_i, imag_j) = conv_in.shape\\n    \\n    if(num_ch_p != num_ch):\\n        print(\"Big mistake! The number of channels in dpool does not match the number of channels in the input!\")\\n\\n    dout = np.zeros((num_ch,imag_i,imag_j))\\n    \\n    for conv in conv_in:\\n        for curr_ch in range(num_ch):\\n            curr_y = curr_y_end = out_y = 0\\n            while(out_y < dp_j):\\n                curr_x = curr_x_end = out_x = 0\\n                curr_y_end = curr_y + p_j\\n                if(curr_y_end > imag_j):\\n                    curr_y_end = imag_j\\n                while(out_x < dp_i):\\n                    curr_x_end = curr_x + p_i\\n                    if(curr_x_end > imag_i):\\n                        curr_x_end = imag_i\\n                    (a,b) = np.unravel_index(np.nanargmax(conv[curr_ch,curr_x:curr_x_end,curr_y:curr_y_end]),conv[curr_ch,curr_x:curr_x_end,curr_y:curr_y_end].shape)\\n                    dout[curr_ch,curr_x+a,curr_y+b] += dpool[curr_ch,out_x,out_y]\\n\\n                    curr_x += stride_i\\n                    out_x += 1\\n                curr_y += stride_j\\n                out_y += 1\\n    return dout\\n\\ndef FbackConnection(dprev,data_in,weight,activation,derivative):\\n    dW = activation(data_in).T.dot(dprev)\\n    db = np.sum(dprev,axis=0,keepdims=True)\\n    dout = dprev.dot(weight.T)*derivative(data_in)\\n    return dout, dW, db\\n    \\n#def Convolution(images, filt, bias, activation, stride_i=1,stride_j=1):\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "def InitializeFilter(dimensions):\n",
    "    return np.random.normal(loc=0,scale=1.0/(np.sqrt(np.prod(dimensions))),size=dimensions)\n",
    "\n",
    "\n",
    "def Convolution(images, filt, bias, activat, stride_i=1,stride_j=1):\n",
    "    (num_filt, num_ch_f, f_i, f_j) = filt.shape\n",
    "    (num_imag, num_ch, imag_i, imag_j) = images.shape\n",
    "    \n",
    "    if(num_ch_f != num_ch):\n",
    "        print(\"Big mistake! The number of channels in the filter does not match the number of channels in the images!\")\n",
    "\n",
    "    out_i = int(np.ceil((imag_i - f_i)/stride_i) + 1)\n",
    "    out_j = int(np.ceil((imag_j - f_j)/stride_j) + 1)\n",
    "    z = np.zeros((num_imag, num_filt, out_i, out_j))\n",
    "    i=0\n",
    "    for image in images:\n",
    "        for curr_f in range(num_filt):\n",
    "            curr_y = curr_y_end = out_y = 0\n",
    "            f_y_end = f_j + 1\n",
    "            while(out_y < out_j):\n",
    "                curr_x = curr_x_end = out_x = 0\n",
    "                curr_y_end = curr_y + f_j\n",
    "                if(curr_y_end > imag_j):\n",
    "                    curr_y_end = imag_j\n",
    "                    f_y_end = curr_y_end - curr_y\n",
    "                f_x_end = f_i + 1\n",
    "                while(out_x < out_i):\n",
    "                    curr_x_end = curr_x + f_i\n",
    "                    if(curr_x_end > imag_i):\n",
    "                        curr_x_end = imag_i\n",
    "                        f_x_end = curr_x_end - curr_x\n",
    "                    z[i,curr_f,out_x,out_y] = np.sum(image[:,curr_x:curr_x_end,curr_y:curr_y_end]*filt[curr_f,:,0:f_x_end,0:f_y_end]) + bias[curr_f]\n",
    "                    curr_x += stride_i\n",
    "                    out_x += 1\n",
    "                curr_y += stride_j\n",
    "                out_y += 1\n",
    "        i+=1\n",
    "    return z,activat(z)\n",
    "\n",
    "def FMaxPool(images,pool_i=2,pool_j=2,stride_i=2,stride_j=2):\n",
    "    (num_imag, num_ch, imag_i, imag_j) = images.shape\n",
    "\n",
    "    out_i = int(np.ceil((imag_i - pool_i)/stride_i)) + 1\n",
    "    out_j = int(np.ceil((imag_j - pool_j)/stride_j)) + 1\n",
    "    z = np.zeros((num_imag, num_ch, out_i, out_j))\n",
    "    index = np.zeros((num_imag, num_ch, out_i, out_j))\n",
    "    \n",
    "    i=0\n",
    "    for image in images:\n",
    "        for curr_chan in range(num_ch):\n",
    "            curr_y = out_y = 0\n",
    "            while(out_y < out_j):\n",
    "                curr_x = out_x = 0\n",
    "                curr_y_end = curr_y + pool_j\n",
    "                if(curr_y_end > imag_j):\n",
    "                    current_y_end = imag_j\n",
    "                while(out_x < out_i):\n",
    "                    curr_x_end = curr_x + pool_i\n",
    "                    if(curr_x_end > imag_i):\n",
    "                        curr_x_end = imag_i\n",
    "                    z[i,curr_chan,out_x,out_y] = np.max(image[curr_chan,curr_x:curr_x_end,curr_y:curr_y_end])\n",
    "                    index[i,curr_chan,out_x,out_y] = np.argmax(image[curr_chan,curr_x:curr_x_end,curr_y:curr_y_end])\n",
    "                    curr_x += stride_i\n",
    "                    out_x +=1\n",
    "                curr_y += stride_j\n",
    "                out_y += 1\n",
    "        i+=1\n",
    "    return z,index\n",
    "\n",
    "def FConnection(images,weights,bias,activation):\n",
    "    z = images.dot(weights) + bias\n",
    "    return z, activation(z)\n",
    "\n",
    "def FbackConvolution(dprev,conv_in,filt,derivative,stride_i=1,stride_j=1):\n",
    "    (num_filt, num_ch_f, f_i, f_j) = filt.shape\n",
    "    (num_imag, num_ch, imag_i, imag_j) = conv_in.shape\n",
    "    \n",
    "    if(num_ch_f != num_ch):\n",
    "        print(\"Big mistake! The number of channels in the filter does not match the number of channels in the input!\")\n",
    "\n",
    "    dout = np.zeros((num_ch,imag_i,imag_j))\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dbias = np.zeros((num_filt,1))\n",
    "    \n",
    "    for conv in conv_in:\n",
    "        dout_conv = np.zeros((num_ch,imag_i,imag_j))\n",
    "        for curr_f in range(num_filt):\n",
    "            curr_y = out_y = 0\n",
    "            f_y_end = f_i\n",
    "            while(curr_y + f_j < imag_j + 1):\n",
    "                curr_x = out_x = 0\n",
    "                curr_y_end = curr_y + f_j\n",
    "                if(curr_y_end > imag_j):\n",
    "                    curr_y_end = imag_j\n",
    "                    f_y_end = curr_y_end - curr_y\n",
    "                f_x_end = f_j\n",
    "                while(curr_x + f_i < imag_i + 1): \n",
    "                    curr_x_end = curr_x + f_i\n",
    "                    if(curr_x_end > imag_i):\n",
    "                        curr_x_end = imag_i\n",
    "                        f_x_end = curr_x_end - curr_i\n",
    "                    dout_conv[:,curr_x:curr_x_end,curr_y:curr_y_end] += dprev[curr_f, out_x, out_y] * filt[curr_f,:,:f_x_end,:f_y_end]\n",
    "                    dfilt[curr_f,:,:f_x_end,:f_y_end] += dprev[curr_f,out_x,out_y] * conv[:,curr_x:curr_x_end,curr_y:curr_y_end]\n",
    "\n",
    "                    curr_x += stride_i\n",
    "                    out_x +=1\n",
    "                curr_y += stride_j\n",
    "                out_y += 1\n",
    "            dbias[curr_f] = np.sum(dprev[curr_f])\n",
    "        dout += dout_conv*derivative(conv)\n",
    "\n",
    "    return dout,dfilt,dbias\n",
    "\n",
    "def FbackMaxPool(dpool, conv_in, p_i=2, p_j=2, stride_i=2, stride_j=2):\n",
    "    (num_ch_p, dp_i, dp_j) = dpool.shape\n",
    "    (num_imag, num_ch, imag_i, imag_j) = conv_in.shape\n",
    "    \n",
    "    if(num_ch_p != num_ch):\n",
    "        print(\"Big mistake! The number of channels in dpool does not match the number of channels in the input!\")\n",
    "\n",
    "    dout = np.zeros((num_ch,imag_i,imag_j))\n",
    "    \n",
    "    for conv in conv_in:\n",
    "        for curr_ch in range(num_ch):\n",
    "            curr_y = curr_y_end = out_y = 0\n",
    "            while(out_y < dp_j):\n",
    "                curr_x = curr_x_end = out_x = 0\n",
    "                curr_y_end = curr_y + p_j\n",
    "                if(curr_y_end > imag_j):\n",
    "                    curr_y_end = imag_j\n",
    "                while(out_x < dp_i):\n",
    "                    curr_x_end = curr_x + p_i\n",
    "                    if(curr_x_end > imag_i):\n",
    "                        curr_x_end = imag_i\n",
    "                    (a,b) = np.unravel_index(np.nanargmax(conv[curr_ch,curr_x:curr_x_end,curr_y:curr_y_end]),conv[curr_ch,curr_x:curr_x_end,curr_y:curr_y_end].shape)\n",
    "                    dout[curr_ch,curr_x+a,curr_y+b] += dpool[curr_ch,out_x,out_y]\n",
    "\n",
    "                    curr_x += stride_i\n",
    "                    out_x += 1\n",
    "                curr_y += stride_j\n",
    "                out_y += 1\n",
    "    return dout\n",
    "\n",
    "def FbackConnection(dprev,data_in,weight,activation,derivative):\n",
    "    dW = activation(data_in).T.dot(dprev)\n",
    "    db = np.sum(dprev,axis=0,keepdims=True)\n",
    "    dout = dprev.dot(weight.T)*derivative(data_in)\n",
    "    return dout, dW, db\n",
    "    \n",
    "#def Convolution(images, filt, bias, activation, stride_i=1,stride_j=1):\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
