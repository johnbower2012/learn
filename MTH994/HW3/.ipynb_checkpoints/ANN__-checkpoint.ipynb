{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "def read_dataset(feature_file, label_file):\n",
    "    ''' Read data set in *.csv to data frame in Pandas'''\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values # convert values in dataframe to numpy array (features)\n",
    "    y = df_y.values # convert values in dataframe to numpy array (label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def normalize_features(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler #import libaray\n",
    "    scaler = StandardScaler() # call an object function\n",
    "    scaler.fit(X_train) # calculate mean, std in X_train\n",
    "    X_train_norm = scaler.transform(X_train) # apply normalization on X_train\n",
    "    X_test_norm = scaler.transform(X_test) # we use the same normalization on X_test\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "\n",
    "def one_hot_encoder(y_train, y_test):\n",
    "    ''' convert label to a vector under one-hot-code fashion '''\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_train)\n",
    "    y_train_ohe = lb.transform(y_train)\n",
    "    y_test_ohe = lb.transform(y_test)\n",
    "    return y_train_ohe, y_test_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input layer has 64 neurons: $x_1, x_2, ..., x_{64}$.\n",
    "\n",
    "## We construct a deep neural network with 1 hidden layer.\n",
    "\n",
    "## In 1st hidden layer:\n",
    "\n",
    "## $z_1 = xW_1 + b_1$\n",
    "\n",
    "## $W_1\\in \\mathbb{R}^{64\\times N_1}$ ,$N_1$ : # of neurons in this hidden layer\n",
    "\n",
    "## $b_1 \\in \\mathbb{R}^{N_1}$, $z_1 \\in \\mathbb{R}^{N_1}$\n",
    "\n",
    "## Use $\\tanh$ as an activation function\n",
    "\n",
    "## $f_1 = \\tanh(z_1) \\in \\mathbb{R}^{N_1}$\n",
    "\n",
    "## In Output layer:\n",
    "\n",
    "## $z_2 = f_1W_2 + b_2$\n",
    "\n",
    "## $W_2 \\in \\mathbb{R}^{N_1\\times 10}, b_2 \\in \\mathbb{R}^{10}$, $z_2 \\in \\mathbb{R}^{10}$\n",
    "\n",
    "## Use softmax function to get probability for each class\n",
    "\n",
    "## $\\hat{y} = \\mathbf{softmax}(z_2)$\n",
    "\n",
    "## $\\displaystyle \\mathbf{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation\n",
    "\n",
    "## Loss function (cross-entropy loss)\n",
    "\n",
    "## $L = -\\sum_n\\sum_{i\\in C} y_{n, i}\\log(\\hat{y}_{n, i})$\n",
    "\n",
    "## We need to consider the following derivatives \n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial W_1}, \\displaystyle\\frac{\\partial L}{\\partial b_1},\n",
    "\\displaystyle\\frac{\\partial L}{\\partial W_2}, \\displaystyle\\frac{\\partial L}{\\partial b_2}$\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial{\\hat{y}}}\\cdot\\frac{\\partial\\hat{y}}{\\partial{z_2}}\n",
    "\\cdot\\frac{\\partial z_2}{\\partial W_2} = f_1^T (\\hat{y}-y)  $\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial{\\hat{y}}}\\cdot\\frac{\\partial\\hat{y}}{\\partial{z_2}}\n",
    "\\cdot\\frac{\\partial z_2}{\\partial b_2} = \\hat{y}-y$\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial{\\hat{y}}}\\cdot\\frac{\\partial\\hat{y}}{\\partial{z_2}}\n",
    "\\cdot\\frac{\\partial z_2}{\\partial f_1}\\cdot \\frac{\\partial f_1}{\\partial z_1}\\cdot \\frac{\\partial z_1}{\\partial W_1} =\n",
    "x^T\\left[(1-f^2_1)(\\hat{y}-y)W_2^T\\right]$\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial{\\hat{y}}}\\cdot\\frac{\\partial\\hat{y}}{\\partial{z_2}}\n",
    "\\cdot\\frac{\\partial z_2}{\\partial f_1}\\cdot \\frac{\\partial f_1}{\\partial z_1}\\cdot \\frac{\\partial z_1}{\\partial b_1} =\n",
    "(1-f^2_1))(\\hat{y}-y)W_2^T$\n",
    "\n",
    "## Let $d_1 = (1-f^2_1)(\\hat{y}-y)W_2^T, d_2 = \\hat{y}-y$\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial W_2} = f_1^T d_2$\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial b_2} = d_2$\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial W_1} = x^T d_1$\n",
    "\n",
    "## $\\displaystyle\\frac{\\partial L}{\\partial b_1} = d_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class varlayer_NN:\n",
    "    def __init__(self, DATA, DACT, DLOSS, hidden_layer=[100], lr=0.01):\n",
    "        self.X = X # features\n",
    "        self.y = y # labels (targets) in one-hot-encoder\n",
    "        self.activation = DACT['F']\n",
    "        self.derivative = DACT['D']\n",
    "        self.lossfunction = DLOSS['F']\n",
    "        self.lossderivative = DLOSS['D']\n",
    "        self.regularization_parameter = DLOSS['RP']\n",
    "        self.hidden_layer = hidden_layer # number of neuron in the hidden layer\n",
    "        # In this example, we only consider 1 hidden layer\n",
    "        self.lr = lr # learning rate\n",
    "        # Initialize weights\n",
    "        self.in_nn = X.shape[1] # number of neurons in the inpute layer              \n",
    "        self.out_nn = y.shape[1]\n",
    "        \n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.W.append(np.random.randn(self.in_nn,self.hidden_layer[0])/np.sqrt(self.in_nn))\n",
    "        self.b.append(np.zeros((1,self.hidden_layer[0])))\n",
    "        for i in range(0,len(self.hidden_layer)-1):\n",
    "            self.W.append(np.random.randn(self.hidden_layer[i],self.hidden_layer[i+1])/np.sqrt(self.hidden_layer[i]))\n",
    "            self.b.append(np.zeros((1,self.hidden_layer[i+1])))\n",
    "\n",
    "        self.W.append(np.random.randn(self.hidden_layer[-1],self.out_nn)/np.sqrt(self.hidden_layer[-1]))\n",
    "        self.b.append(np.zeros((1,self.out_nn)))\n",
    "        self.C = [self.W,self.b]\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        # hidden layer\n",
    "        self.z = []\n",
    "        self.f = []\n",
    "        ## z_1 = xW_1 + b_1\n",
    "        self.z.append(np.dot(self.X, self.W[0]) + self.b[0])\n",
    "        ## activation function :  f_1 = \\tanh(z_1)\n",
    "        self.f.append(self.activation(self.z[0]))\n",
    "        for i in range(0,len(self.hidden_layer)-1):\n",
    "            self.z.append(np.dot(self.f[i], self.W[i+1]) + self.b[i+1])\n",
    "            self.f.append(self.activation(self.z[-1]))\n",
    "        # output layer\n",
    "        ## z_2 = f_1W_2 + b_2\n",
    "        self.z.append(np.dot(self.f[-1], self.W[-1]) + self.b[-1])\n",
    "        #\\hat{y} = softmax}(z_2)$\n",
    "        self.y_hat = softmax(self.z[-1])\n",
    "        \n",
    "        \n",
    "    def back_propagation(self):\n",
    "        d = []\n",
    "        d.insert(0,self.lossderivative(self.y,self.y_hat,self.regularization_parameter,self.C))\n",
    "        dW = []\n",
    "        db = []\n",
    "        for i in range(len(self.hidden_layer)):\n",
    "            dW.insert(0,self.f[-1-i].T.dot(d[0]))\n",
    "            db.insert(0,np.sum(d[0],axis=0,keepdims=True))\n",
    "            d.insert(0,self.derivative(self.f[-1-i])*(d[0].dot(self.W[-1-i].T)))\n",
    "        dW.insert(0,self.X.T.dot(d[0]))\n",
    "        db.insert(0,np.sum(d[0],axis=0,keepdims=True))\n",
    "        \n",
    "        \n",
    "        # Update the gradident descent\n",
    "        if(self.regularization_parameter==None):\n",
    "            for i in range(len(self.W)):            \n",
    "                self.W[i] = self.W[i] - self.lr * dW[i]\n",
    "                self.b[i] = self.b[i] - self.lr * db[i]\n",
    "        else:\n",
    "            for i in range(len(self.W)):\n",
    "                self.W[i] = self.W[i] - self.lr * dW[i]\n",
    "                if(self.W[i].all()!=0):\n",
    "                    self.W[i] = self.W[i] - self.lr * self.W[i]/np.sqrt(np.sum(self.W[i]*self.W[i]))\n",
    "                self.b[i] = self.b[i] - self.lr * db[i]\n",
    "                if(self.b[i].all()!=0):\n",
    "                    self.b[i] = self.b[i] - self.lr * self.b[i]/np.sqrt(np.sum(self.b[i]*self.b[i]))\n",
    "\n",
    "\n",
    "    def callloss(self):\n",
    "        #  $L = -\\sum_n\\sum_{i\\in C} y_{n, i}\\log(\\hat{y}_{n, i})$\n",
    "        # calculate y_hat\n",
    "        self.feed_forward()\n",
    "        self.loss = self.lossfunction(self.y,self.y_hat,self.regularization_parameter,self.C)\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Use feed forward to calculat y_hat_test\n",
    "        # hidden layer\n",
    "        ## z_1 = xW_1 + b_1\n",
    "        z = np.dot(X_test, self.W[0]) + self.b[0]\n",
    "        for i in range(1,len(self.hidden_layer)+1):\n",
    "            f = self.activation(z)\n",
    "            z = np.dot(f,self.W[i]) + self.b[i]\n",
    "        y_hat_test = softmax(z)\n",
    "\n",
    "        # the rest is similar to the logistic regression\n",
    "        labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        num_test_samples = X_test.shape[0]\n",
    "        # find which index gives us the highest probability\n",
    "        ypred = np.zeros(num_test_samples, dtype=int)\n",
    "        for i in range(num_test_samples):\n",
    "            ypred[i] = labels[np.argmax(y_hat_test[i,:])]\n",
    "        return ypred\n",
    "    \n",
    "    def train(self,epochs):\n",
    "        for i in range(epochs):\n",
    "            self.feed_forward()\n",
    "            self.back_propagation()\n",
    "            self.callloss()\n",
    "        \n",
    "def softmax(z):\n",
    "    exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True)) # for stablility\n",
    "    # keepdims = True means that the output's dimension is the same as of z\n",
    "    softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "    return softmax_scores\n",
    "\n",
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTIVATIONS\n",
    "def TANH(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def TANH_dx(X):\n",
    "    return 1 - np.tanh(X)*np.tanh(X)\n",
    "\n",
    "def RELU(X):\n",
    "    dx = np.copy(X)\n",
    "    dx[X<0] = 0\n",
    "    return dx\n",
    "\n",
    "def RELU_dx(X):\n",
    "    dx = np.copy(X)\n",
    "    dx[dx<0]=0\n",
    "    dx[dx>0]=1\n",
    "    return dx\n",
    "\n",
    "#LOSSES\n",
    "def CrossEntropyLoss(y,yhat,regularization_parameter=None,C=None):\n",
    "    return -np.sum(y*np.log(yhat + 1e-6))\n",
    "\n",
    "def CrossEntropyLoss_dx(y,yhat,regularization_parameter=None,C=None): #What?\n",
    "    return yhat - y\n",
    "\n",
    "def RegularizedLoss(y,yhat,regularization_parameter,C):\n",
    "    loss = 0\n",
    "    hinge = 1 - y*yhat\n",
    "    hinge[hinge<0]=0\n",
    "    loss = np.sum(hinge)\n",
    "    loss *= regularization_parameter\n",
    "    temp = 0\n",
    "    for item in C:\n",
    "        for elem in item:\n",
    "            temp += np.sum(elem*elem)\n",
    "    return loss + np.sqrt(temp)\n",
    "\n",
    "def RegularizedLoss_dx(y,yhat,regularization_parameter,C):\n",
    "    hinge = 1 - y*yhat\n",
    "    summed = np.sum(y*yhat,axis=1,keepdims=True)\n",
    "    loss = np.zeros(hinge.shape)\n",
    "    for i in range(hinge.shape[0]):\n",
    "        for j in range(hinge.shape[1]):\n",
    "            if(hinge[i][j]>0):\n",
    "                loss[i][j] = yhat[i][j]*(summed[i,:] - y[i][j])\n",
    "    loss *= regularization_parameter\n",
    "    return loss\n",
    "\n",
    "def RegularizedLoss2(y,yhat,regularization_parameter,C):\n",
    "    loss = 0\n",
    "    hinge = 1 - y*yhat\n",
    "    hinge = np.sum(hinge,axis=1,keepdims=True)\n",
    "    hinge[hinge<0]=0\n",
    "    loss = np.sum(hinge)\n",
    "    loss *= regularization_parameter\n",
    "    temp = 0\n",
    "    for item in C:\n",
    "        for elem in item:\n",
    "            temp += np.sum(elem*elem)\n",
    "    return loss + np.sqrt(temp)\n",
    "\n",
    "def RegularizedLoss_dx2(y,yhat,regularization_parameter,C):\n",
    "    hinge = 1 - y*yhat\n",
    "    hinge = np.sum(hinge,axis=1,keepdims=True)\n",
    "    summed = np.sum(y*yhat,axis=1,keepdims=True)\n",
    "    loss = np.zeros(y.shape)\n",
    "    for i in range(hinge.shape[0]):\n",
    "        if(hinge[i]>0):\n",
    "            loss[i] = yhat[i]*(summed[i] - y[i])\n",
    "    loss *= regularization_parameter\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "def RunOne(dlib,layerslist,epochs,lr=0.1):\n",
    "    data = dlib['DATA']\n",
    "    dact = dlib['DACT']\n",
    "    dloss = dlib['DLOSS']\n",
    "    print('Running TwoLayer NN with:\\n   layer1:',layerslist[0],'\\n   epochs:', epochs,'\\n   lr:', lr)\n",
    "    print('   ',dlib['DACT'])\n",
    "    print('   ',dlib['DLOSS'],'\\n')\n",
    "    for l1 in layerslist[0]:\n",
    "            hidden_layer=[l1]\n",
    "            myNN = varlayer_NN(data['X_train_norm'], data['y_train_ohe'], dact, dloss, hidden_layer, lr) \n",
    "            myNN.train(epochs)\n",
    "            y_pred = myNN.predict(data['X_test_norm'])\n",
    "            print('Accuracy of our model_TEST: %f%%' % (100*accuracy(y_pred, data['y_test'].ravel())))\n",
    "            print('   NEURONS: %d'%(l1))\n",
    "\n",
    "def RunTwo(dlib,layerslist,epochs,lr=0.1):\n",
    "    data = dlib['DATA']\n",
    "    dact = dlib['DACT']\n",
    "    dloss = dlib['DLOSS']\n",
    "    print('Running TwoLayer NN with:\\n   layer1:',layerslist[0],'\\n   layer2:',layerslist[1],'\\n   epochs:', epochs,'\\n   lr:', lr)\n",
    "    print('   ',dlib['DACT'])\n",
    "    print('   ',dlib['DLOSS'],'\\n')\n",
    "    for l1 in layerslist[0]:\n",
    "        for l2 in layerslist[1]:\n",
    "            hidden_layer=[l1,l2]\n",
    "            myNN = varlayer_NN(data['X_train_norm'], data['y_train_ohe'], dact, dloss, hidden_layer, lr) \n",
    "            myNN.train(epochs)\n",
    "            y_pred = myNN.predict(data['X_test_norm'])\n",
    "            print('Accuracy of our model_TEST: %f%%' % (100*accuracy(y_pred, data['y_test'].ravel())))\n",
    "            print('   NEURONS: %d %d'%(l1,l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_digit(feature_vector):\n",
    "    dim = int(np.sqrt(len(feature_vector)))\n",
    "    plt.gray()\n",
    "    plt.matshow(feature_vector.reshape(dim,dim))\n",
    "    plt.show()\n",
    "\n",
    "def plot_test(index1,index2,Clearn,DATA):\n",
    "    for index in range(index1,index2):\n",
    "        print('Image:',index)\n",
    "        plot_digit(DATA['X_test'][index])\n",
    "        ypred = Clearn.predict(DATA['X_test_norm'][index].reshape(1,-1))\n",
    "        print('Label:', int(DATA['y_test'][index]))\n",
    "        print('Prediction:',int(ypred))\n",
    "        print('')\n",
    "        \n",
    "def tally_matrix(y_test,pred):\n",
    "    total=np.zeros(10)\n",
    "    tal=np.zeros((10,10))\n",
    "    for i in range(len(y_test)):\n",
    "        y=int(y_test[i])\n",
    "        p=int(pred[i])\n",
    "        tal[y][p]+=1\n",
    "        total[y]+=1\n",
    "    for index in range(tal.shape[0]):\n",
    "        tal[index] *= 100/total[index]\n",
    "    return tal\n",
    "        \n",
    "def tally_matrix_no_diag(y_test,pred):\n",
    "    total=np.zeros(10)\n",
    "    tal=np.zeros((10,10))\n",
    "    for i in range(len(y_test)):\n",
    "        y=int(y_test[i])\n",
    "        p=int(pred[i])\n",
    "        if(y!=p):\n",
    "            tal[y][p]+=1\n",
    "        total[y]+=1\n",
    "    for index in range(tal.shape[0]):\n",
    "        tal[index] *= 100/total[index]\n",
    "    return tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "X_train_digits, y_train_digits = read_dataset('Digits_X_train.csv', 'Digits_y_train.csv')\n",
    "X_test_digits, y_test_digits = read_dataset('Digits_X_test.csv', 'Digits_y_test.csv')\n",
    "X_train_norm_digits, X_test_norm_digits = normalize_features(X_train_digits, X_test_digits)\n",
    "y_train_ohe_digits, y_test_ohe_digits = one_hot_encoder(y_train_digits, y_test_digits)\n",
    "\n",
    "X_train_mnist, y_train_mnist = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
    "X_test_mnist, y_test_mnist = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')\n",
    "X_train_norm_mnist, X_test_norm_mnist = normalize_features(X_train_mnist, X_test_mnist)\n",
    "y_train_ohe_mnist, y_test_ohe_mnist = one_hot_encoder(y_train_mnist, y_test_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "#ACTIVATIONS\n",
    "DRELU = {'F':RELU,'D':RELU_dx}\n",
    "DTANH = {'F':TANH,'D':TANH_dx}\n",
    "\n",
    "#LOSS\n",
    "DENTR = {'F':CrossEntropyLoss,'D':CrossEntropyLoss_dx,'RP':0.1}\n",
    "DREGU = {'F':RegularizedLoss,'D':RegularizedLoss_dx,'RP':0.1}\n",
    "DREGU2 = {'F':RegularizedLoss2,'D':RegularizedLoss_dx2,'RP':0.1}\n",
    "\n",
    "#DATA\n",
    "DDIG = {'X_train':X_train_digits,'X_train_norm':X_train_norm_digits,'y_train':y_train_digits,'y_train_ohe':y_train_ohe_digits,\\\n",
    "        'X_test':X_test_digits,'X_test_norm':X_test_norm_digits,'y_test':y_test_digits,'y_test_ohe':y_test_ohe_digits}\n",
    "DMNI = {'X_train':X_train_mnist,'X_train_norm':X_train_norm_mnist,'y_train':y_train_mnist,'y_train_ohe':y_train_ohe_mnist,\\\n",
    "        'X_test':X_test_mnist,'X_test_norm':X_test_norm_mnist,'y_test':y_test_mnist,'y_test_ohe':y_test_ohe_mnist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TwoLayer NN with:\n",
      "   layer1: [20, 50, 80] \n",
      "   epochs: 200 \n",
      "   lr: 0.01\n",
      "    {'F': <function RELU at 0x11ed44840>, 'D': <function RELU_dx at 0x11ed448c8>}\n",
      "    {'F': <function RegularizedLoss2 at 0x11ed44b70>, 'D': <function RegularizedLoss_dx2 at 0x11ed44bf8>, 'RP': 0.1} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA = DDIG\n",
    "DACT = DRELU\n",
    "DLOSS = DREGU2\n",
    "DLIB = {'DATA':DATA,\\\n",
    "        'DACT':DRELU,\\\n",
    "        'DLOSS':DREGU2}\n",
    "\n",
    "L1 = [20,50,80]\n",
    "L2 = [20,50,80]\n",
    "layers = [L1,L2]\n",
    "epoch_num = 200\n",
    "lr=0.01\n",
    "\n",
    "\n",
    "layers = [L1]\n",
    "RunOne(DLIB,layers,epoch_num,lr)\n",
    "print('')\n",
    "layers = [L1,L2]\n",
    "RunTwo(DLIB,layers,epoch_num,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TwoLayer NN with:\n",
      "   layer1: [20, 50, 80] \n",
      "   layer2: [20, 50, 80] \n",
      "   epochs: 200 \n",
      "   lr: 0.01\n",
      "    {'F': <function RELU at 0x11ed44840>, 'D': <function RELU_dx at 0x11ed448c8>}\n",
      "    {'F': <function RegularizedLoss2 at 0x11ed44b70>, 'D': <function RegularizedLoss_dx2 at 0x11ed44bf8>, 'RP': 0.1} \n",
      "\n",
      "Accuracy of our model_TEST: 89.800000%\n",
      "   NEURONS: 20 20\n",
      "Accuracy of our model_TEST: 89.000000%\n",
      "   NEURONS: 20 50\n",
      "Accuracy of our model_TEST: 90.000000%\n",
      "   NEURONS: 20 80\n",
      "Accuracy of our model_TEST: 90.000000%\n",
      "   NEURONS: 50 20\n",
      "Accuracy of our model_TEST: 89.800000%\n",
      "   NEURONS: 50 50\n",
      "Accuracy of our model_TEST: 91.200000%\n",
      "   NEURONS: 50 80\n",
      "Accuracy of our model_TEST: 90.000000%\n",
      "   NEURONS: 80 20\n",
      "Accuracy of our model_TEST: 90.400000%\n",
      "   NEURONS: 80 50\n",
      "Accuracy of our model_TEST: 91.200000%\n",
      "   NEURONS: 80 80\n"
     ]
    }
   ],
   "source": [
    "DATA = DMNI\n",
    "DACT = DRELU\n",
    "DLOSS = DREGU2\n",
    "DLIB = {'DATA':DATA,\\\n",
    "        'DACT':DRELU,\\\n",
    "        'DLOSS':DREGU2}\n",
    "\n",
    "L1 = [20,50,80]\n",
    "L2 = [20,50,80]\n",
    "layers = [L1,L2]\n",
    "epoch_num = 200\n",
    "lr=0.01\n",
    "\n",
    "RunTwo(DLIB,layers,epoch_num,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 92.666667%\n",
      "   NEURONS: 20 20\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 96.222222%\n",
      "   NEURONS: 20 50\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 94.000000%\n",
      "   NEURONS: 20 80\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 94.888889%\n",
      "   NEURONS: 50 20\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 97.111111%\n",
      "   NEURONS: 50 50\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 93.777778%\n",
      "   NEURONS: 50 80\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 75.777778%\n",
      "   NEURONS: 80 20\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 74.000000%\n",
      "   NEURONS: 80 50\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 96.888889%\n",
      "   NEURONS: 80 80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "DATA = DDIG\n",
    "DACT = DRELU\n",
    "DLOSS = DREGU2\n",
    "\n",
    "L1 = [20,50,80]\n",
    "L2 = [20,50,80]\n",
    "DLAYERS = [L1,L2]\n",
    "epoch_num = 200\n",
    "#np.set_printoptions(precision=2)\n",
    "#plt.gray()\n",
    "\n",
    "for l1 in L1:\n",
    "    for l2 in L2:\n",
    "        hidden_layer=[l1,l2]\n",
    "        myNN = varlayer_NN(DATA['X_train_norm'], DATA['y_train_ohe'], DACT, DLOSS, hidden_layer, lr=0.1) \n",
    "        for i in range(epoch_num):\n",
    "            myNN.feed_forward()\n",
    "            myNN.back_propagation()\n",
    "            myNN.callloss()\n",
    "            #if ((i+1)%20 == 0):\n",
    "            #    print('epoch = %d, current loss = %.5f' % (i+1, myNN.loss))\n",
    "            #    y_pred = myNN.predict(DATA['X_test_norm'])\n",
    "            #    print('   Accuracy of our model_TEST: %d %d: %f%%' % (l1,l2,100*accuracy(y_pred, DATA['y_test'].ravel())))\n",
    "            #    y_pred = myNN.predict(DATA['X_train_norm'])\n",
    "            #    print('   Accuracy of our model_TRAIN: %d %d: %f%%' % (l1,l2,100*accuracy(y_pred, DATA['y_train'].ravel())))\n",
    "        y_pred = myNN.predict(DATA['X_test_norm'])\n",
    "        print('--FINAL--:')\n",
    "        print('Accuracy of our model_TEST: %f%%' % (100*accuracy(y_pred, DATA['y_test'].ravel())))\n",
    "        print('   NEURONS: %d %d'%(l1,l2))\n",
    "        #y_pred = myNN.predict(DATA['X_train_norm'])\n",
    "        #print('   Accuracy of our model_TRAIN: %d %d: %f%%' % (l1,l2,100*accuracy(y_pred, DATA['y_train'].ravel())))\n",
    "        #tally_mat=tally_matrix(DATA['y_test'],y_pred)\n",
    "        #print(tally_mat)\n",
    "        #plt.matshow(tally_mat)\n",
    "        #tally_mat=tally_matrix_no_diag(DATA['y_test'],y_pred)\n",
    "        #plt.matshow(tally_mat)\n",
    "        #plt.show()    \n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 90.000000%\n",
      "   NEURONS: 20 20\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 89.400000%\n",
      "   NEURONS: 20 50\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 88.600000%\n",
      "   NEURONS: 20 80\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 89.800000%\n",
      "   NEURONS: 50 20\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 89.200000%\n",
      "   NEURONS: 50 50\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 89.600000%\n",
      "   NEURONS: 50 80\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 89.800000%\n",
      "   NEURONS: 80 20\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 91.200000%\n",
      "   NEURONS: 80 50\n",
      "\n",
      "--FINAL--:\n",
      "Accuracy of our model_TEST: 89.400000%\n",
      "   NEURONS: 80 80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "DATA = DMNI\n",
    "DACT = DRELU\n",
    "DLOSS = DREGU2\n",
    "\n",
    "L1 = [20,50,80]\n",
    "L2 = [20,50,80]\n",
    "DLAYERS = [L1,L2]\n",
    "epoch_num = 200\n",
    "#np.set_printoptions(precision=2)\n",
    "#plt.gray()\n",
    "\n",
    "for l1 in L1:\n",
    "    for l2 in L2:\n",
    "        hidden_layer=[l1,l2]\n",
    "        myNN = varlayer_NN(DATA['X_train_norm'], DATA['y_train_ohe'], DACT, DLOSS, hidden_layer, lr=0.005) \n",
    "        for i in range(epoch_num):\n",
    "            myNN.feed_forward()\n",
    "            myNN.back_propagation()\n",
    "            myNN.callloss()\n",
    "            #if ((i+1)%50 == 0):\n",
    "            #    print('epoch = %d, current loss = %.5f' % (i+1, myNN.loss))\n",
    "            #    y_pred = myNN.predict(DATA['X_test'])\n",
    "            #    print('   Accuracy of our model_TEST: %d %d: %f%%' % (l1,l2,100*accuracy(y_pred, DATA['y_test'].ravel())))\n",
    "            #    y_pred = myNN.predict(DATA['X_train_norm'])\n",
    "            #    print('   Accuracy of our model_TRAIN: %d %d: %f%%' % (l1,l2,100*accuracy(y_pred, DATA['y_train'].ravel())))\n",
    "        y_pred = myNN.predict(DATA['X_test_norm'])\n",
    "        print('--FINAL--:')\n",
    "        print('Accuracy of our model_TEST: %f%%' % (100*accuracy(y_pred, DATA['y_test'].ravel())))\n",
    "        print('   NEURONS: %d %d'%(l1,l2))\n",
    "        #y_pred = myNN.predict(DATA['X_train'])\n",
    "        #print('   Accuracy of our model_TRAIN: %d %d: %f%%' % (l1,l2,100*accuracy(y_pred, DATA['y_train'].ravel())))\n",
    "        #tally_mat=tally_matrix(DATA['y_test'],y_pred)\n",
    "        #print(tally_mat)\n",
    "        #plt.matshow(tally_mat)\n",
    "        #tally_mat=tally_matrix_no_diag(DATA['y_test'],y_pred)\n",
    "        #plt.matshow(tally_mat)\n",
    "        #plt.show()    \n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADhxJREFUeJzt3X+oVeWex/HPp3L6owysyMzJqekX1sWrdbCCGIpIHAu1ArkS5UBkfxgUTDAWQf3RYAzWTFAUmnIduLch8tqPW+QtEbJfcjWsLOeahDaaHqkgLZMh/c4fZ/m0p+t51t5n7bPXOvp+weHss757r/V1efz4rLXXfpYjQgAgSSfU3QCA5iAQACQEAoCEQACQEAgAEgIBQFJLINiebvsvtrfZXlhHDzm2t9v+xPYm2xsa0M9y23ttb25ZdrrtN21/Xnwf07D+HrG9q9iHm2zPqLG/c22vtf2Z7U9t31ssb8Q+zPTX833oXl+HYPtESVsl3SBpp6Q/S5obEZ/1tJEM29sl9UXE13X3Ikm2/0HS95L+MyJ+VSz7N0nfRsRjRaiOiYh/aVB/j0j6PiIW19FTK9vjJI2LiA9tj5a0UdJsSf+kBuzDTH9z1ON9WMcIYaqkbRHxRUT8r6T/kjSrhj5GjIh4W9K3v1g8S9KK4vEKDfwC1WKQ/hojInZHxIfF4/2Stkgar4bsw0x/PVdHIIyX9D8tP+9UTX/4jJD0J9sbbc+vu5lBjI2I3cXjPZLG1tnMIO6x/XFxSFHbIU0r2+dJmiJpvRq4D3/Rn9TjfchJxaO7JiIul/SPkhYUQ+LGioHjvqZdg/6MpAskTZa0W9Lj9bYj2T5V0kpJ90XEvtZaE/bhUfrr+T6sIxB2STq35ee/LZY1RkTsKr7vlbRKA4c5TdNfHHseOQbdW3M//09E9EfEoYg4LGmpat6Htkdp4B/b7yLiD8XixuzDo/VXxz6sIxD+LOki2+fb/htJv5H0Sg19HJXtU4oTO7J9iqRpkjbnX1WLVyTNKx7Pk/Ryjb38lSP/0Ao3q8Z9aNuSlknaEhFPtJQasQ8H66+OfdjzdxkkqXj75D8knShpeUT8a8+bGITtv9fAqECSTpL0+7r7s/28pGslnSmpX9LDkl6S9IKkCZJ2SJoTEbWc2Bukv2s1MNQNSdsl3d1yvN7r/q6RtE7SJ5IOF4sf1MBxeu37MNPfXPV4H9YSCACaiZOKABICAUBCIABICAQACYEAIKk1EBp8WbAk+quqyf01uTepvv7qHiE0+i9F9FdVk/trcm9STf3VHQgAGqTShUm2p0t6UgNXHD4XEY+VPJ+roICaRITLnjPkQBjKRCcEAlCfdgKhyiEDE50Ax5gqgTASJjoB0IGThnsDxdsnTT+jC0DVAqGtiU4iYomkJRLnEICmq3LI0OiJTgB0bsgjhIj4yfY9klbr54lOPu1aZwB6rqcTpHDIANRnuN92BHCMIRAAJAQCgIRAAJAQCAASAgFAQiAASAgEAAmBACAhEAAkBAKAhEAAkBAIABICAUAy7FOo4fhx2mmnZeszZ87M1m+66aZsfc6cOdn6u+++m63PmDEjW9+/f3+2fjxghAAgIRAAJAQCgIRAAJAQCAASAgFAQiAASJiGHW176KGHsvVbb701W580aVI32+nY/Pn5OwouW7asR53Ug2nYAXSEQACQEAgAEgIBQEIgAEgIBAAJgQAgYT4EJAsXLszW77///mx99OjR3Wyn6y677LK6W2i8SoFge7uk/ZIOSfopIvq60RSAenRjhHBdRHzdhfUAqBnnEAAkVQMhJP3J9kbb+QvFATRe1UOGayJil+2zJL1p+78j4u3WJxRBQVgAI0ClEUJE7Cq+75W0StLUozxnSUT0ccIRaL4hB4LtU2yPPvJY0jRJm7vVGIDeq3LIMFbSKttH1vP7iHijK12hFtu2bcvWTz755Gz94MGD2foXX3yRrV955ZXZ+tq1a7P1vr78IPTqq6/O1lEhECLiC0m/7mIvAGrG244AEgIBQEIgAEgIBAAJgQAgIRAAJMyHgOTFF1/M1suuQ5gwYUK2vmjRoo57anX48OFKr3/uuecqvf54wAgBQEIgAEgIBAAJgQAgIRAAJAQCgIRAAJA4Inq3Mbt3G8Mx5/3338/WJ02alK2ff/752frevXs77mkkiQiXPYcRAoCEQACQEAgAEgIBQEIgAEgIBAAJgQAgYT4E9ExxD49BLViwIFu//PLLs/WnnnoqWz/WrzPoBkYIABICAUBCIABICAQACYEAICEQACQEAoCE6xDQM2eddVa2/uSTT1Za/4EDByq9Hm2MEGwvt73X9uaWZafbftP258X3McPbJoBeaOeQ4beSpv9i2UJJayLiIklrip8BjHClgRARb0v69heLZ0laUTxeIWl2l/sCUIOhnlQcGxG7i8d7JI3tUj8AalT5pGJERG7yVNvzJc2vuh0Aw2+oI4R+2+Mkqfg+6MfIImJJRPRFRN8QtwWgR4YaCK9Imlc8nifp5e60A6BOpYcMtp+XdK2kM23vlPSwpMckvWD7Tkk7JM0ZziZxbCi7L0JVq1atGtb1Hw9KAyEi5g5Sur7LvQCoGZcuA0gIBAAJgQAgIRAAJAQCgIRAAJAwHwJ6ZuXKlZVe/+qrr2brH330UaX1gxECgBYEAoCEQACQEAgAEgIBQEIgAEgIBAAJ1yGga+66665sfdy4cZXWv2jRomz90KFDldYPRggAWhAIABICAUBCIABICAQACYEAICEQACRch9AgEyZMyNa//PLLHnUyNOPHj8/WIwa9458kac2aNdn61q1bO+4JnWGEACAhEAAkBAKAhEAAkBAIABICAUBCIABIXPbecFc3ZvduYzWYO3dutv7AAw9k62eccUa2/s0333TcUydsZ+tlvysXXnhhtv7DDz9Uev13332XrSMvIvJ/wWpjhGB7ue29tje3LHvE9i7bm4qvGVWbBVC/dg4Zfitp+lGW/3tETC6+Xu9uWwDqUBoIEfG2pG970AuAmlU5qXiP7Y+LQ4oxXesIQG2GGgjPSLpA0mRJuyU9PtgTbc+3vcH2hiFuC0CPDCkQIqI/Ig5FxGFJSyVNzTx3SUT0RUTfUJsE0BtDCgTbrfNp3yxp82DPBTBylM6HYPt5SddKOtP2TkkPS7rW9mRJIWm7pLuHscfGmD17dra+YsWKbP3EE0+stP2zzz670uvLnHBC/v+Hw4cPV1p/2X0TJk6cmK1/8MEHlbaPcqWBEBFHu9pm2TD0AqBmXLoMICEQACQEAoCEQACQEAgAEgIBQMJ8CB3YuHFjtj558uRK63/22Wez9XXr1lVa//TpR/vQ6s/uuOOObH24f1fK5nu45ZZbsvV33nmnm+0cc7oyHwKA4weBACAhEAAkBAKAhEAAkBAIABICAUBS+vHn48msWbOy9UmTJlVa/44dO7L1svs27Nu3L1u/7rrrsvUpU6Zk62X6+/uz9TvvvDNbL7tO49FHH83WFy1alK3fcMMN2frBgwezdTBCANCCQACQEAgAEgIBQEIgAEgIBAAJgQAgYT6EFuvXr8/W+/ryN58qu2/B9ddfn62X3Xfg9ttvz9affvrpbH3UqFHZ+p49e7L1G2+8MVvftGlTtj527Nhs/auvvsrWy8ycOTNbf+211yqtf6RjPgQAHSEQACQEAoCEQACQEAgAEgIBQEIgAEiYD6HFgQMHKr2+7L4CZdcRLF68OFu/4oorOu6p1datW7P1iRMnVlp/mbL5HF5//fVsfcaMGdn6VVddla0f79chtKN0hGD7XNtrbX9m+1Pb9xbLT7f9pu3Pi+9jhr9dAMOpnUOGnyT9c0RcKukqSQtsXyppoaQ1EXGRpDXFzwBGsNJAiIjdEfFh8Xi/pC2SxkuaJWlF8bQVkmYPV5MAeqOjk4q2z5M0RdJ6SWMjYndR2iMpf6E6gMZr+6Si7VMlrZR0X0Tss3/+nERExGAfXLI9X9L8qo0CGH5tjRBsj9JAGPwuIv5QLO63Pa6oj5O092ivjYglEdEXEfmPCgKoXTvvMljSMklbIuKJltIrkuYVj+dJern77QHopdL5EGxfI2mdpE8kHfnA/4MaOI/wgqQJknZImhMR35asq9HzIZxzzjnZ+urVq7P1Sy+9tJvtdOy9997L1qdNm5at//jjj91sp2OXXHJJtl7253vjjTey9dtuu63jno4l7cyHUHoOISLekTTYivIzfgAYUbh0GUBCIABICAQACYEAICEQACQEAoCE+zJ0YPz48dn6W2+9la1ffPHF2frSpUuz9Z07d1Z6fX9/f7bedGXXiUydOjVbf+mll7rZzojDfRkAdIRAAJAQCAASAgFAQiAASAgEAAmBACDhOgTgOMF1CAA6QiAASAgEAAmBACAhEAAkBAKAhEAAkBAIABICAUBCIABICAQACYEAICEQACQEAoCEQACQlAaC7XNtr7X9me1Pbd9bLH/E9i7bm4qvGcPfLoDhVDpBiu1xksZFxIe2R0vaKGm2pDmSvo+IxW1vjAlSgNq0M0HKSW2sZLek3cXj/ba3SMrfwgjAiNTROQTb50maIml9sege2x/bXm57TJd7A9BjbQeC7VMlrZR0X0Tsk/SMpAskTdbACOLxQV433/YG2xu60C+AYdTWJKu2R0n6o6TVEfHEUernSfpjRPyqZD2cQwBq0pVJVm1b0jJJW1rDoDjZeMTNkjYPpUkAzdHOuwzXSFon6RNJh4vFD0qaq4HDhZC0XdLdxQnI3LoYIQA1aWeEwH0ZgOME92UA0BECAUBCIABICAQACYEAICEQACQEAoCEQACQEAgAEgIBQEIgAEgIBAAJgQAgIRAAJAQCgKR01uUu+1rSjpafzyyWNRX9VdPk/prcm9T9/v6unSf1dIKUv9q4vSEi+mproAT9VdPk/prcm1RffxwyAEgIBABJ3YGwpObtl6G/aprcX5N7k2rqr9ZzCACape4RAoAGIRAAJAQCgIRAAJAQCACS/wMidh5LsZTv9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n",
      "Prediction: 2\n",
      "\n",
      "Image: 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n",
      "Prediction: 0\n",
      "\n",
      "Image: 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n",
      "Prediction: 6\n",
      "\n",
      "Image: 58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n",
      "Prediction: 7\n",
      "\n",
      "Image: 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n",
      "Prediction: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index1=55\n",
    "index2=60\n",
    "DATA=DMNI\n",
    "plot_test(index1,index2,myNN,DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
